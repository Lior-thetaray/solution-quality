{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import Window, functions as f\n",
    "\n",
    "from lib.entity_resolution import EntityResolution, MatchState\n",
    "from thetaray.api.context import init_context\n",
    "from thetaray.api.dataset import dataset_functions\n",
    "from thetaray.api.solution.explainability_validator import validate_explainabilities\n",
    "\n",
    "context = init_context(execution_date=datetime(1970, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "ds_account = dataset_functions.read(context, \"account\").drop(\"name\", \"address\", \"country\")\n",
    "ds_card = dataset_functions.read(context, \"card\")\n",
    "ds_client = dataset_functions.read(context, \"client\")\n",
    "ds_disp_owner = dataset_functions.read(context, \"disp_owner\")\n",
    "ds_disp_disponent = dataset_functions.read(context, \"disp_disponent\")\n",
    "ds_district = dataset_functions.read(context, \"district\")\n",
    "ds_loan = dataset_functions.read(context, \"loan\")\n",
    "ds_transaction = dataset_functions.read(context, \"transaction\", to_job_ts=context.execution_date)\n",
    "ds_order = dataset_functions.read(context, \"order\")\n",
    "ds_country_risk = dataset_functions.read(context, \"country_risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_disp = ds_disp_owner.union(ds_disp_disponent)\n",
    "features = ds_account.join(ds_disp, \"account_id\", \"outer\")\n",
    "features = features.withColumnRenamed(\"date\", \"date_acct\").join(\n",
    "    ds_loan.withColumnRenamed(\"date\", \"date_loan\"),\n",
    "    \"account_id\",\n",
    "    \"left\",\n",
    ")\n",
    "features = features.withColumnRenamed(\"district_id\", \"district_id_bank\").join(\n",
    "    ds_client.withColumnRenamed(\"district_id\", \"district_id_client\"),\n",
    "    \"client_id\",\n",
    "    \"outer\",\n",
    ")\n",
    "features = features.withColumnRenamed(\"type\", \"type_disp\").join(\n",
    "    ds_card.withColumnRenamed(\"type\", \"type_card\"),\n",
    "    \"disp_id\",\n",
    "    \"outer\",\n",
    ").drop(\"codes\")\n",
    "features = features.withColumnRenamed(\"date\", \"date_card\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(features.count(), \"total feature records, ie one for each client\")  # should be 5369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = features.filter(features.loan_id.isNotNull())\n",
    "# print(features.count(), \"feature records with a loan; some accts repeated due to multiple clients on same acct\")  # should be 827\n",
    "# print(features.select(\"account_id\").distinct().count(), \"feature records with a loan and unique account_id\")  # should be 682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trans_acctdate = ds_transaction.join(features.select(\"account_id\", \"date_loan\"), on=\"account_id\")\n",
    "trans_acctdate = trans_acctdate.withColumn(\"datediff\", f.datediff(f.col(\"date_loan\"), f.col(\"date\")))    \n",
    "trans_acctdate = trans_acctdate.filter(f.col(\"datediff\") > 0)\n",
    "\n",
    "windows = []\n",
    "aggs = []\n",
    "for x in range(1, 6 + 1):\n",
    "    trans_acctdate = trans_acctdate.withColumn(f\"M{x}\", f.when(f.col(\"datediff\") < x * 30, True).otherwise(False))\n",
    "    w = Window().partitionBy(\"account_id\", f\"M{x}\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    windows.insert(x, w)\n",
    "    aggs.extend(\n",
    "        [\n",
    "            f.min(\"balance\").over(w).alias(f\"min{x}\"),\n",
    "            f.max(\"balance\").over(w).alias(f\"max{x}\"),\n",
    "            f.mean(\"balance\").over(w).alias(f\"mean{x}\"),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "monbalstats = trans_acctdate.groupBy(\"account_id\", \"balance\", \"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\").agg(*aggs)\n",
    "    \n",
    "monbalstats = monbalstats.filter(\n",
    "    (f.col(\"M1\") == True)\n",
    "    | (f.col(\"M2\") == True)\n",
    "    | (f.col(\"M3\") == True)\n",
    "    | (f.col(\"M4\") == True)\n",
    "    | (f.col(\"M5\") == True)\n",
    "    | (f.col(\"M6\") == True)\n",
    ").drop(\"balance\").dropDuplicates()\n",
    "\n",
    "aggs = []\n",
    "for x in range(1, 6 + 1):\n",
    "    monbalstats = monbalstats.withColumn(\n",
    "        f\"min{x}\",\n",
    "        f.when(f.col(f\"M{x}\") == True, f.col(f\"min{x}\")).otherwise(f.lit(None)),\n",
    "    ).withColumn(\n",
    "        f\"max{x}\",\n",
    "        f.when(f.col(f\"M{x}\") == True, f.col(f\"max{x}\")).otherwise(f.lit(None)),\n",
    "    ).withColumn(\n",
    "        f\"mean{x}\",\n",
    "        f.when(f.col(f\"M{x}\") == True, f.col(f\"mean{x}\")).otherwise(f.lit(None)),\n",
    "    )\n",
    "    aggs.extend(\n",
    "        [\n",
    "            f.max(f\"min{x}\").alias(f\"min{x}\"),\n",
    "            f.max(f\"max{x}\").alias(f\"max{x}\"),\n",
    "            f.max(f\"mean{x}\").alias(f\"mean{x}\"),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "monbalstats = monbalstats.groupBy(\"account_id\").agg(*aggs)\n",
    "\n",
    "features = features.join(monbalstats, on=\"account_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert response var `status` = {A,B,C,D} to `response` = {0,1} (AC good, BD bad):\n",
    "features = features.withColumn(\n",
    "    \"response\",\n",
    "    f.when(f.col(\"status\") == \"A\", 1).when(f.col(\"status\") == \"C\", 1).otherwise(0),\n",
    ").drop(\"status\")\n",
    "\n",
    "# There are credit card features, but not all clients have cards so these features can be Nan,\n",
    "# which isn't acceptable in the modeling.  Let's create a `has_card`={0,1} feature, drop the\n",
    "# date the card was opened, and then below we'll still use the type_card feature in a way\n",
    "# that avoids NaNs.\n",
    "features = features.withColumn(\n",
    "    \"has_card\",\n",
    "    f.when(f.col(\"issued\").isNotNull(), 1).otherwise(0),\n",
    ").drop(\"issued\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = features.drop(\n",
    "    \"tr_timestamp\",\n",
    "    \"tr_timestamp_client\",\n",
    "    \"tr_timestamp_bank\",\n",
    "    \"tr_timestamp_y\",\n",
    "    \"tr_timestamp_x\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = features.withColumn(\n",
    "    \"has_card\",\n",
    "    f.col(\"has_card\").cast(\"long\"),\n",
    ").withColumn(\n",
    "    \"response\",\n",
    "    f.col(\"response\").cast(\"long\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLAINABILITY (WIDGETS)\n",
    "\n",
    "features = features.withColumn('population_amount', f.col('amount') * (f.rand() * .25 + .5))\n",
    "account_name_address = dataset_functions.read(context, 'account').select('account_id', 'name', 'address')\n",
    "features = features.join(account_name_address, on='account_id', how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLAINABILITY (WIDGETS) [MVP-51612]\n",
    "\n",
    "# High Risk Countries\n",
    "account_country = dataset_functions.read(context, 'account').select('account_id', 'country')\n",
    "\n",
    "ds_transaction = ds_transaction.join(\n",
    "    account_country,\n",
    "    ds_transaction['receiver_id'] == account_country['account_id']\n",
    ").select(ds_transaction['*'], account_country['country'].alias('receiver_country'))\n",
    "\n",
    "ds_transaction = ds_transaction.join(\n",
    "    ds_country_risk,\n",
    "    ds_transaction['receiver_country'] == ds_country_risk['country_code']\n",
    ").select(ds_transaction['*'], ds_country_risk['risk'].alias('receiver_country_risk'))\n",
    "\n",
    "high_risk_countries_explainability = (\n",
    "    ds_transaction\n",
    "    .groupby('account_id', 'receiver_country', 'receiver_country_risk')\n",
    "    .agg(f.count('*').alias('count'), f.sum('amount').alias('sum'))\n",
    "    .groupby('account_id')\n",
    "    .agg(f.collect_list(\n",
    "        f.struct(\n",
    "            f.col('receiver_country').alias('ct'),\n",
    "            f.col('receiver_country_risk').alias('cr'),\n",
    "            f.col('count').alias('c'),\n",
    "            f.col('sum').alias('s')\n",
    "        )).alias('high_risk_countries_explainability')\n",
    "    )\n",
    "    .select(\n",
    "        'account_id',\n",
    "        f.arrays_overlap(\n",
    "            f.transform(f.col('high_risk_countries_explainability'), lambda x: x['cr']),\n",
    "            f.array(f.lit('High'), f.lit('Very High'))\n",
    "        ).alias('high_risk_countries'),  # high_risk_countries is True if the account sent money to high risk countries\n",
    "        f.to_json(\n",
    "            f.create_map(\n",
    "                f.lit('data'),\n",
    "                f.col('high_risk_countries_explainability')\n",
    "            )\n",
    "        ).alias('high_risk_countries_explainability')\n",
    "    )\n",
    ")\n",
    "\n",
    "features = features.join(\n",
    "    high_risk_countries_explainability.alias('EXPL'),\n",
    "    on='account_id'\n",
    ").select(features['*'], 'EXPL.high_risk_countries', 'EXPL.high_risk_countries_explainability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLAINABILITY (WIDGETS) [MVP-51612]\n",
    "\n",
    "# Keyword Matches\n",
    "keyword_matches_explainability = (\n",
    "    ds_transaction\n",
    "    .filter(~f.isnan('keyword_group'))\n",
    "    .groupby('account_id', 'keyword_group')\n",
    "    .agg(f.count('*').alias('count'), f.sum('amount').alias('sum'))\n",
    "    .groupby('account_id')\n",
    "    .agg(f.collect_list(\n",
    "        f.struct(\n",
    "            f.col('keyword_group').alias('kw'),\n",
    "            f.col('count').alias('c'),\n",
    "            f.col('sum').alias('s')\n",
    "        )).alias('keyword_matches_explainability')\n",
    "    )\n",
    "    .select(\n",
    "        'account_id',\n",
    "        f.lit(True).alias('keyword_matches'),  # keyword_matches is always True here because of isnan filter above\n",
    "        f.to_json(\n",
    "            f.create_map(\n",
    "                f.lit('data'),\n",
    "                f.col('keyword_matches_explainability')\n",
    "            )\n",
    "        ).alias('keyword_matches_explainability')\n",
    "    )\n",
    ")\n",
    "\n",
    "features = features.join(\n",
    "    keyword_matches_explainability.alias('EXPL'),\n",
    "    on='account_id',\n",
    "    how='left'\n",
    ").select(features['*'], 'EXPL.keyword_matches', 'EXPL.keyword_matches_explainability')\n",
    "\n",
    "features = features.withColumn('keyword_matches', f.when(f.col('keyword_matches').isNull(), False).otherwise(f.col('keyword_matches')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.coalesce(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = validate_explainabilities(\n",
    "    context,\n",
    "    features,\n",
    "    'tr_analysis',\n",
    "    show_all_columns=False # False -> only expl and validation columns, True -> full input df with the validation columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Load parties from graph and join it to aggregated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Load graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "grouping_identifier = \"party_id\"\n",
    "primary_identifier = \"account_id\"\n",
    "primary_entities_list = \"party_accounts\"\n",
    "\n",
    "er = EntityResolution(context=context, graph_id=\"public\")\n",
    "graph_account_party = er.load_graph(\n",
    "    grouping_identifier=grouping_identifier,\n",
    "    primary_identifier=primary_identifier,\n",
    "    states=[MatchState.AUTO_CONFIRM, MatchState.MANUAL_CONFIRM],\n",
    ")\n",
    "graph_account_party = graph_account_party.select(grouping_identifier, primary_identifier)\n",
    "graph_account_party.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Aggregate all accounts into primary_entities_list column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "map_column = f.create_map(f.lit(primary_identifier), primary_identifier)\n",
    "\n",
    "# Aggregate the DataFrame to collect list of map for each primary_identifier\n",
    "mapped_df = (\n",
    "    graph_account_party.withColumn(primary_entities_list, map_column)\n",
    "    .groupBy(grouping_identifier)\n",
    "    .agg(f.collect_list(primary_entities_list).alias(primary_entities_list))\n",
    ")\n",
    "\n",
    "# Convert the primary_entities_list column to a JSON string\n",
    "mapped_df = mapped_df.withColumn(primary_entities_list, f.to_json(primary_entities_list))\n",
    "\n",
    "# Join the aggregated df\n",
    "graph_account_party = graph_account_party.join(f.broadcast(mapped_df), grouping_identifier)\n",
    "\n",
    "# Put space after colon in order to be compatible with trace queries\n",
    "graph_account_party = graph_account_party.withColumn(primary_entities_list, f.regexp_replace(primary_entities_list, \":\", \": \"))\n",
    "\n",
    "graph_account_party.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "features = features.join(\n",
    "    f.broadcast(graph_account_party),\n",
    "    on=primary_identifier,\n",
    "    how=\"left\",\n",
    ").drop(graph_account_party[primary_identifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_functions.write(context, features, \"wrangling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "context.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
