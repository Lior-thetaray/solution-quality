{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fa237e-8525-49e3-a3c2-4a8b187af9ef",
   "metadata": {},
   "source": [
    "### Init Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a09ce3b-f18b-4027-b946-0065c3b81d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thetaray.api.context import init_context\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(message)s')\n",
    "\n",
    "with open('/thetaray/git/solutions/domains/demo_merchant/config/spark_config.yaml') as spark_config_file:\n",
    "    spark_config = yaml.load(spark_config_file, yaml.FullLoader)['spark_config_a']\n",
    "\n",
    "context = init_context(\n",
    "    execution_date=datetime(1970, 2, 1),\n",
    "    spark_conf=spark_config,\n",
    "    # spark_master='local[*]',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200bc84-f919-4e06-bb3d-c60d693903d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T12:03:39.026382Z",
     "iopub.status.busy": "2025-05-06T12:03:39.025702Z",
     "iopub.status.idle": "2025-05-06T12:03:39.028658Z",
     "shell.execute_reply": "2025-05-06T12:03:39.028210Z",
     "shell.execute_reply.started": "2025-05-06T12:03:39.026354Z"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ba67b-9ec8-4cf8-81d7-c5193355815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from domains.demo_merchant.datasets.customer_monthly import customer_monthly_dataset\n",
    "from domains.demo_merchant.datasets.customers import customers_dataset\n",
    "from domains.demo_merchant.datasets.customer_insights import customer_insights_dataset\n",
    "from domains.demo_merchant.datasets.transactions import transactions_dataset\n",
    "from domains.demo_merchant.evaluation_flows.ef import evaluation_flow as ef\n",
    "from domains.demo_merchant.graphs.graph import graph\n",
    "\n",
    "from thetaray.api.evaluation import unblock_evaluation_for_execution_date\n",
    "from thetaray.common.data_environment import DataEnvironment\n",
    "\n",
    "from sqlalchemy import text\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8089fd1d-3847-4c2a-ae5c-6a2e7b974d0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T12:03:39.026382Z",
     "iopub.status.busy": "2025-05-06T12:03:39.025702Z",
     "iopub.status.idle": "2025-05-06T12:03:39.028658Z",
     "shell.execute_reply": "2025-05-06T12:03:39.028210Z",
     "shell.execute_reply.started": "2025-05-06T12:03:39.026354Z"
    }
   },
   "source": [
    "### Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de0667-5ad1-49e7-ba91-b722736af0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.drop_spark_execution_partition(customer_monthly_dataset().identifier, context.execution_date, data_environment=DataEnvironment.PUBLIC)\n",
    "context.drop_spark_execution_partition(customers_dataset().identifier, context.execution_date, data_environment=DataEnvironment.PUBLIC)\n",
    "context.drop_spark_execution_partition(transactions_dataset().identifier, context.execution_date, data_environment=DataEnvironment.PUBLIC)\n",
    "context.drop_spark_execution_partition(customer_insights_dataset().identifier, context.execution_date, data_environment=DataEnvironment.PUBLIC)\n",
    "context.drop_spark_execution_partition(ef().identifier, context.execution_date, data_environment=DataEnvironment.PUBLIC)\n",
    "unblock_evaluation_for_execution_date(context, ef().identifier, data_environment=DataEnvironment.PUBLIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a830a1-205a-4de6-9908-fe0598517c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark  # noqa\n",
    "except NameError:\n",
    "    try:\n",
    "        spark = context.spark\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            spark = context.spark_session\n",
    "        except AttributeError:\n",
    "            # Último recurso: crea/recupera una sesión\n",
    "            from pyspark.sql import SparkSession\n",
    "            spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd665dad-b4a0-4311-9cf3-4a5cc8d81a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CLEAR TOTAL (sin depender de execution_date/particiones) ---\n",
    "\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def ensure_spark(context):\n",
    "    if 'spark' in globals():\n",
    "        try:\n",
    "            spark.range(1).count()\n",
    "            return spark\n",
    "        except Exception:\n",
    "            pass\n",
    "    for attr in (\"spark\", \"spark_session\"):\n",
    "        if hasattr(context, attr) and getattr(context, attr) is not None:\n",
    "            try:\n",
    "                getattr(context, attr).range(1).count()\n",
    "                return getattr(context, attr)\n",
    "            except Exception:\n",
    "                pass\n",
    "    if hasattr(context, \"get_spark_session\"):\n",
    "        try:\n",
    "            s = context.get_spark_session()\n",
    "            s.range(1).count()\n",
    "            return s\n",
    "        except Exception:\n",
    "            pass\n",
    "    from pyspark.sql import SparkSession\n",
    "    return SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark = ensure_spark(context)\n",
    "\n",
    "short_names = [\n",
    "    customer_monthly_dataset().identifier,\n",
    "    customers_dataset().identifier,\n",
    "    transactions_dataset().identifier,\n",
    "    customer_insights_dataset().identifier,\n",
    "]\n",
    "\n",
    "def find_tables(short_name: str):\n",
    "    hits = []\n",
    "    try:\n",
    "        dbs = [r.databaseName for r in spark.sql(\"SHOW DATABASES\").collect()]\n",
    "    except Exception:\n",
    "        dbs = [spark.catalog.currentDatabase()]\n",
    "    for db in dbs:\n",
    "        try:\n",
    "            if spark.sql(f\"SHOW TABLES IN {db} LIKE '{short_name}'\").count() > 0:\n",
    "                hits.append(f\"{db}.{short_name}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return hits\n",
    "\n",
    "full_tables = []\n",
    "for s in short_names:\n",
    "    found = find_tables(s)\n",
    "    if not found:\n",
    "        print(f\"[WARN] No se encontró la tabla '{s}' en ningún DB\")\n",
    "    else:\n",
    "        for f in found:\n",
    "            print(\"[FOUND]\", f)\n",
    "        full_tables.extend(found)\n",
    "\n",
    "# Parar streams/caché\n",
    "for q in spark.streams.active:\n",
    "    try: q.stop()\n",
    "    except: pass\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "def drop_all_partitions_if_any(full_name: str):\n",
    "    # Intentar listar y dropear todas las particiones (si la tabla es particionada)\n",
    "    try:\n",
    "        parts = spark.sql(f\"SHOW PARTITIONS {full_name}\").collect()\n",
    "        if parts:\n",
    "            print(f\" - Dropping {len(parts)} partitions\")\n",
    "            for row in parts:\n",
    "                # row[0] es una cadena como \"job_ts=2025-08-01/...\" o \"year=2025/month=08/...\"\n",
    "                spec = \", \".join([f\"{kv.split('=')[0]}='{kv.split('=')[1]}'\" for kv in row[0].split('/')])\n",
    "                try:\n",
    "                    spark.sql(f\"ALTER TABLE {full_name} DROP IF EXISTS PARTITION ({spec})\")\n",
    "                except Exception as e:\n",
    "                    print(\"   * DROP PARTITION failed ->\", e)\n",
    "    except AnalysisException:\n",
    "        # Tabla no particionada o sin soporte de SHOW PARTITIONS\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(\" - SHOW PARTITIONS failed ->\", e)\n",
    "\n",
    "def strong_clear_table(full_name: str):\n",
    "    print(\"\\nClearing:\", full_name)\n",
    "    ok = False\n",
    "    # 1) TRUNCATE\n",
    "    try:\n",
    "        spark.sql(f\"TRUNCATE TABLE {full_name}\")\n",
    "        ok = True\n",
    "        print(\" - TRUNCATE ok\")\n",
    "    except Exception as e:\n",
    "        print(\" - TRUNCATE failed ->\", e)\n",
    "\n",
    "    # 2) Si TRUNCATE no funcionó, intenta dropear todas las particiones\n",
    "    if not ok:\n",
    "        drop_all_partitions_if_any(full_name)\n",
    "\n",
    "    # 3) DELETE total por si quedan restos (Delta/Hive)\n",
    "    try:\n",
    "        spark.sql(f\"DELETE FROM {full_name} WHERE 1=1\")\n",
    "        print(\" - DELETE all ok\")\n",
    "    except Exception as e:\n",
    "        print(\" - DELETE failed ->\", e)\n",
    "\n",
    "    # 4) Verificar conteo; si siguen filas, último recurso DROP TABLE\n",
    "    cnt = None\n",
    "    try:\n",
    "        cnt = spark.sql(f\"SELECT COUNT(*) c FROM {full_name}\").collect()[0][\"c\"]\n",
    "        print(f\" - After clear count = {cnt}\")\n",
    "    except AnalysisException:\n",
    "        print(\" - Table no accesible (posiblemente ya no existe)\")\n",
    "    if cnt is not None and cnt > 0:\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {full_name}\")\n",
    "            print(\" - DROPPED table (último recurso)\")\n",
    "        except Exception as e:\n",
    "            print(\" - DROP failed ->\", e)\n",
    "\n",
    "for t in full_tables:\n",
    "    strong_clear_table(t)\n",
    "\n",
    "# Verificación\n",
    "errors = []\n",
    "for t in full_tables:\n",
    "    try:\n",
    "        c = spark.sql(f\"SELECT COUNT(*) c FROM {t}\").collect()[0][\"c\"]\n",
    "        print(f\"[CHECK] {t}: {c} filas\")\n",
    "        if c != 0:\n",
    "            errors.append(f\"{t} aún tiene {c} filas\")\n",
    "    except AnalysisException:\n",
    "        print(f\"[CHECK] {t}: no existe (OK si esperabas dropearla)\")\n",
    "    except Exception as e:\n",
    "        errors.append(f\"{t}: error al contar -> {e}\")\n",
    "\n",
    "assert not errors, \"CLEAR CHECK FAILED -> \" + \" | \".join(errors)\n",
    "True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce764e-0a8e-4ca0-8b4a-76922b35b0b1",
   "metadata": {},
   "source": [
    "### Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527b300-28a9-4c27-9e45-597e24672814",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_namespace = os.environ[\"SHARED_NAMESPACE\"]\n",
    "\n",
    "engine_cdd = create_engine(\n",
    "    f\"postgresql+psycopg2://postgres:postgres@postgres.{shared_namespace}.svc.cluster.local:5432/cdd\"\n",
    ")\n",
    "\n",
    "engine_apps = create_engine(\n",
    "    f\"postgresql+psycopg2://postgres:postgres@postgres.{shared_namespace}.svc.cluster.local:5432/apps_tmdemo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df70cc-10d4-4307-a07b-fddf5f410396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This for trying no to drop a table does not exists (!)\n",
    "# =========================\n",
    "# CLEAR DATA (IDEMPOTENT)\n",
    "# =========================\n",
    "\n",
    "# --- Step 1: Imports and helper try_truncate ---\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "from psycopg2.errors import UndefinedTable\n",
    "\n",
    "def try_truncate(conn, default_schema: str, table_or_qualified: str):\n",
    "    \"\"\"\n",
    "    Run TRUNCATE TABLE safely:\n",
    "    - If the table does not exist (UndefinedTable), skip and continue.\n",
    "    - Accepts either 'table' or 'schema.table'. If schema not provided, uses default_schema.\n",
    "    \"\"\"\n",
    "    if \".\" in table_or_qualified:\n",
    "        schema, table = table_or_qualified.split(\".\", 1)\n",
    "    else:\n",
    "        schema, table = default_schema, table_or_qualified\n",
    "\n",
    "    try:\n",
    "        conn.execute(text(f'TRUNCATE TABLE \"{schema}\".\"{table}\"'))\n",
    "    except ProgrammingError as e:\n",
    "        # Ignore if relation does not exist\n",
    "        if isinstance(getattr(e, \"orig\", None), UndefinedTable):\n",
    "            print(f\"[SKIP] {schema}.{table} does not exist (ok).\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# =========================\n",
    "# 1) solution_sonar\n",
    "# =========================\n",
    "schema_name = \"solution_sonar\"\n",
    "eval_flow_id = ef().identifier  # <- provided by your environment\n",
    "\n",
    "# Table prefixes for each eval flow\n",
    "eval_flow_table_prefixes = [\n",
    "    \"activity_\",\n",
    "    \"activity_risk_\",\n",
    "    \"test_activity_suppressed_\",\n",
    "    \"test_activity_risk_\",\n",
    "    \"test_activity_risk_thin_\",\n",
    "    \"activity_risk_thin_\",\n",
    "    \"test_activity_\",\n",
    "    \"activity_suppressed_\",\n",
    "]\n",
    "\n",
    "# Truncate by prefixes\n",
    "with engine_cdd.connect() as conn:\n",
    "    for prefix in eval_flow_table_prefixes:\n",
    "        table_full = prefix + eval_flow_id\n",
    "        try_truncate(conn, schema_name, table_full)\n",
    "\n",
    "# Truncate project datasets\n",
    "with engine_cdd.connect() as conn:\n",
    "    try_truncate(conn, schema_name, transactions_dataset().identifier)\n",
    "    try_truncate(conn, schema_name, customer_monthly_dataset().identifier)\n",
    "    try_truncate(conn, schema_name, customers_dataset().identifier)\n",
    "    try_truncate(conn, schema_name, customer_insights_dataset().identifier)\n",
    "    try_truncate(conn, schema_name, f\"tr_nodes_{graph().identifier}\")\n",
    "    try_truncate(conn, schema_name, f\"tr_edges_{graph().identifier}\")\n",
    "\n",
    "# =========================\n",
    "# 2) apps_tmdemo + investigation_center\n",
    "# =========================\n",
    "schema_name = \"apps_tmdemo\"\n",
    "dpv = 'dpv:demo_merchant'\n",
    "\n",
    "# Get the alert table name mapped to this DPV\n",
    "with engine_cdd.connect() as conn:\n",
    "    result = conn.execute(\n",
    "        text(f'SELECT alert_table_name FROM \"{schema_name}\".rp_mappers WHERE data_permission = :dpv'),\n",
    "        {\"dpv\": dpv}\n",
    "    ).first()\n",
    "\n",
    "if result and result[0]:\n",
    "    alert_table_name = result[0]  # e.g. tr_alert_table_1755790351163 (could also be \"schema.table\")\n",
    "\n",
    "    # Truncate in CDD (apps_tmdemo)\n",
    "    with engine_cdd.connect() as conn:\n",
    "        try_truncate(conn, schema_name, alert_table_name)\n",
    "\n",
    "    # Truncate in Apps/IC (may not exist; helper will skip safely)\n",
    "    with engine_apps.connect() as conn:\n",
    "        try_truncate(conn, \"investigation_center\", alert_table_name)\n",
    "\n",
    "    # Delete rp_alerts rows for this mapper\n",
    "    mapper = alert_table_name.split('_')[-1]\n",
    "    with engine_cdd.connect() as conn:\n",
    "        conn.execute(\n",
    "            text(f'DELETE FROM \"{schema_name}\".rp_alerts WHERE alert_mapper_identifier = :m'),\n",
    "            {\"m\": mapper}\n",
    "        )\n",
    "else:\n",
    "    print(f\"[SKIP] No row in {schema_name}.rp_mappers for {dpv} (ok).\")\n",
    "\n",
    "# =========================\n",
    "# 3) Spark checks (unchanged from your original)\n",
    "# =========================\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Ensure Spark\n",
    "try:\n",
    "    spark.range(1).count()\n",
    "except Exception:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Short names of demo merchant datasets\n",
    "short_names = [\n",
    "    customer_monthly_dataset().identifier,\n",
    "    customers_dataset().identifier,\n",
    "    transactions_dataset().identifier,\n",
    "    customer_insights_dataset().identifier,\n",
    "]\n",
    "\n",
    "# Locate tables in all DBs\n",
    "def find_tables(short_name: str):\n",
    "    hits = []\n",
    "    try:\n",
    "        dbs = [r.databaseName for r in spark.sql(\"SHOW DATABASES\").collect()]\n",
    "    except Exception:\n",
    "        dbs = [spark.catalog.currentDatabase()]\n",
    "    for db in dbs:\n",
    "        try:\n",
    "            if spark.sql(f\"SHOW TABLES IN {db} LIKE '{short_name}'\").count() > 0:\n",
    "                hits.append(f\"{db}.{short_name}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return hits\n",
    "\n",
    "errors = []\n",
    "found_any = False\n",
    "for s in short_names:\n",
    "    fq_names = find_tables(s)\n",
    "    if not fq_names:\n",
    "        print(f\"[CHECK] {s}: not found in any DB (OK if dropped).\")\n",
    "        continue\n",
    "    found_any = True\n",
    "    for fq in fq_names:\n",
    "        try:\n",
    "            cnt = spark.sql(f\"SELECT COUNT(*) c FROM {fq}\").collect()[0][\"c\"]\n",
    "            print(f\"[CHECK] {fq}: {cnt} rows\")\n",
    "            if cnt != 0:\n",
    "                errors.append(f\"{fq} still has {cnt} rows\")\n",
    "        except AnalysisException as e:\n",
    "            print(f\"[CHECK] {fq}: not accessible (maybe dropped) -> {e}\")\n",
    "        except Exception as e:\n",
    "            errors.append(f\"{fq}: error while counting -> {e}\")\n",
    "\n",
    "# Assert final (fail only if rows remain)\n",
    "if not found_any:\n",
    "    print(\"[CHECK] No project tables found (OK if clear dropped everything).\")\n",
    "else:\n",
    "    assert not errors, \"CLEAR CHECK FAILED -> \" + \" | \".join(errors)\n",
    "print(\"Verification finished.\")\n",
    "\n",
    "# =========================\n",
    "# 4) API verification (unchanged)\n",
    "# =========================\n",
    "from thetaray.api.dataset import dataset_functions\n",
    "from thetaray.common.data_environment import DataEnvironment\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    eff_dt = getattr(context, \"effective_execution_date\", None) or getattr(context, \"execution_date\", None)\n",
    "    if eff_dt is None:\n",
    "        eff_dt = datetime.utcnow()  # safe fallback\n",
    "    ds = dataset_functions.read(\n",
    "        context,\n",
    "        customer_monthly_dataset().identifier,\n",
    "        from_job_ts=eff_dt,   # do not use 1970-02-01\n",
    "        data_environment=DataEnvironment.PUBLIC\n",
    "    )\n",
    "    c = ds.count()\n",
    "    print(f\"[CHECK API] {customer_monthly_dataset().identifier} @ {eff_dt}: {c} rows\")\n",
    "    assert c == 0, f\"Expected 0, found {c}\"\n",
    "    print(\"API verification OK.\")\n",
    "except Exception as e:\n",
    "    print(f\"[CHECK API] Skipped (table not found or reader empty): {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcacf44-8c13-4249-a282-af802390949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set schema and eval flow to delete\n",
    "# schema_name = \"solution_sonar\"\n",
    "# eval_flow_id = ef().identifier\n",
    "\n",
    "# # Table Prefixes to Truncate for Each Eval Flow\n",
    "# eval_flow_table_prefixes = [\n",
    "#     \"activity_\",\n",
    "#     \"activity_risk_\",\n",
    "#     \"test_activity_suppressed_\",\n",
    "#     \"test_activity_risk_\",\n",
    "#     \"test_activity_risk_thin_\",\n",
    "#     \"activity_risk_thin_\",\n",
    "#     \"test_activity_\",\n",
    "#     \"activity_suppressed_\",\n",
    "# ]\n",
    "\n",
    "# # Truncate\n",
    "# with engine_cdd.connect() as conn:\n",
    "#     for prefix in eval_flow_table_prefixes:\n",
    "#         table_full = prefix + eval_flow_id\n",
    "#         query = text(f\"TRUNCATE TABLE {schema_name}.{table_full}\")\n",
    "#         conn.execute(query)\n",
    "\n",
    "# with engine_cdd.connect() as conn:\n",
    "#     query = text(f\"TRUNCATE TABLE {schema_name}.{transactions_dataset().identifier}\")\n",
    "#     conn.execute(query)\n",
    "#     query = text(f\"TRUNCATE TABLE {schema_name}.{customer_monthly_dataset().identifier}\")\n",
    "#     conn.execute(query)\n",
    "#     query = text(f\"TRUNCATE TABLE {schema_name}.{customers_dataset().identifier}\")\n",
    "#     conn.execute(query)\n",
    "#     query = text(f\"TRUNCATE TABLE {schema_name}.{customer_insights_dataset().identifier}\")\n",
    "#     conn.execute(query)\n",
    "#     query = text(f\"TRUNCATE TABLE {schema_name}.tr_nodes_{graph().identifier}\")\n",
    "#     conn.execute(query)\n",
    "#     query = text(f\"TRUNCATE TABLE {schema_name}.tr_edges_{graph().identifier}\")\n",
    "#     conn.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b0f4c-6ff0-431e-b2dd-417baac3a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set schema and eval flow to delete\n",
    "# schema_name = \"apps_tmdemo\"\n",
    "# dpv = 'dpv:demo_merchant'\n",
    "\n",
    "# with engine_cdd.connect() as conn:\n",
    "#     query = text(f\"SELECT alert_table_name FROM {schema_name}.rp_mappers WHERE data_permission = '{dpv}'\")\n",
    "#     result = conn.execute(query).first()\n",
    "\n",
    "# with engine_cdd.connect() as conn:\n",
    "#     query = text(f\"TRUNCATE TABLE {schema_name}.{result[0]}\")\n",
    "#     conn.execute(query)\n",
    "\n",
    "# with engine_apps.connect() as conn:\n",
    "#     query = text(f\"TRUNCATE TABLE investigation_center.{result[0]}\")\n",
    "#     conn.execute(query)\n",
    "\n",
    "# mapper = result[0].split('_')[-1]\n",
    "\n",
    "# with engine_cdd.connect() as conn:\n",
    "#     query = text(f\"DELETE FROM {schema_name}.rp_alerts WHERE alert_mapper_identifier = '{mapper}'\")\n",
    "#     conn.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b394f5-8a98-42d7-9176-f60175085cc2",
   "metadata": {},
   "source": [
    "### Check if everything was cleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda701c3-fa02-4e9b-8f9d-3ebeb886bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# 1) Asegurar Spark\n",
    "try:\n",
    "    spark.range(1).count()\n",
    "except Exception:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# 2) Short names de demo merchant\n",
    "short_names = [\n",
    "    customer_monthly_dataset().identifier,\n",
    "    customers_dataset().identifier,\n",
    "    transactions_dataset().identifier,\n",
    "    customer_insights_dataset().identifier,\n",
    "]\n",
    "\n",
    "# 3) Localizar tablas en todos los DBs\n",
    "def find_tables(short_name: str):\n",
    "    hits = []\n",
    "    try:\n",
    "        dbs = [r.databaseName for r in spark.sql(\"SHOW DATABASES\").collect()]\n",
    "    except Exception:\n",
    "        dbs = [spark.catalog.currentDatabase()]\n",
    "    for db in dbs:\n",
    "        try:\n",
    "            if spark.sql(f\"SHOW TABLES IN {db} LIKE '{short_name}'\").count() > 0:\n",
    "                hits.append(f\"{db}.{short_name}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return hits\n",
    "\n",
    "errors = []\n",
    "found_any = False\n",
    "for s in short_names:\n",
    "    fq_names = find_tables(s)\n",
    "    if not fq_names:\n",
    "        print(f\"[CHECK] {s}: no existe en ningún DB (OK si fue dropeada).\")\n",
    "        continue\n",
    "    found_any = True\n",
    "    for fq in fq_names:\n",
    "        try:\n",
    "            cnt = spark.sql(f\"SELECT COUNT(*) c FROM {fq}\").collect()[0][\"c\"]\n",
    "            print(f\"[CHECK] {fq}: {cnt} filas\")\n",
    "            if cnt != 0:\n",
    "                errors.append(f\"{fq} aún tiene {cnt} filas\")\n",
    "        except AnalysisException as e:\n",
    "            print(f\"[CHECK] {fq}: no accesible (posible drop) -> {e}\")\n",
    "        except Exception as e:\n",
    "            errors.append(f\"{fq}: error al contar -> {e}\")\n",
    "\n",
    "# 5) Assert final (fallar solo si quedan filas)\n",
    "if not found_any:\n",
    "    print(\"[CHECK] No se encontró ninguna tabla del proyecto (OK si el clear dropeó todo).\")\n",
    "else:\n",
    "    assert not errors, \"CLEAR CHECK FAILED -> \" + \" | \".join(errors)\n",
    "print(\"Verificación terminada.\")\n",
    "\n",
    "# 6) (Opcional) Verificación con API alineada a la fecha efectiva del DAG\n",
    "from thetaray.api.dataset import dataset_functions\n",
    "from thetaray.common.data_environment import DataEnvironment\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    eff_dt = getattr(context, \"effective_execution_date\", None) or getattr(context, \"execution_date\", None)\n",
    "    if eff_dt is None:\n",
    "        eff_dt = datetime.utcnow()  # fallback benigno\n",
    "    ds = dataset_functions.read(\n",
    "        context,\n",
    "        customer_monthly_dataset().identifier,\n",
    "        from_job_ts=eff_dt,   # no uses 1970-02-01\n",
    "        data_environment=DataEnvironment.PUBLIC\n",
    "    )\n",
    "    c = ds.count()\n",
    "    print(f\"[CHECK API] {customer_monthly_dataset().identifier} @ {eff_dt}: {c} filas\")\n",
    "    assert c == 0, f\"Esperaba 0, encontré {c}\"\n",
    "    print(\"Verificación API OK.\")\n",
    "except Exception as e:\n",
    "    print(f\"[CHECK API] Omitido (tabla no existe o lector sin datos): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82828789-d28d-40f7-8b7c-1f4ab29b87a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
