{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19cebc9-333a-4fc0-9b77-f1d2c8669baf",
   "metadata": {},
   "source": [
    "### Init Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040bf097-c103-4473-86e3-f50ce1face7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T15:24:49.206067Z",
     "iopub.status.busy": "2025-08-26T15:24:49.205922Z",
     "iopub.status.idle": "2025-08-26T15:24:59.197251Z",
     "shell.execute_reply": "2025-08-26T15:24:59.196198Z",
     "shell.execute_reply.started": "2025-08-26T15:24:49.206030Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/thetaray/venv/lib64/python3.11/site-packages/starlette/config.py:60: UserWarning: Config file '.env' not found.\n",
      "  warnings.warn(f\"Config file '{env_file}' not found.\")\n",
      "2025-08-26 15:24:58,307:INFO:thetaray.common.logging:start loading solution.....[ load_risks=True , solution_path=/thetaray/git/solutions/domains , settings_path=/thetaray/git/solutions/settings ]\n",
      "2025-08-26 15:24:58,350:WARNING:thetaray.common.logging:Encryption is enabled on dataset demo_human_trafficking_insights, but data encryption is disabled in deployment\n",
      "2025-08-26 15:24:58,380:WARNING:thetaray.common.logging:Encryption is enabled on dataset demo_remittance_customer_insights, but data encryption is disabled in deployment\n",
      "2025-08-26 15:24:58,402:WARNING:thetaray.common.logging:Encryption is enabled on dataset demo_ret_smb_customer_insights, but data encryption is disabled in deployment\n",
      "2025-08-26 15:24:58,443:WARNING:thetaray.common.logging:Encryption is enabled on dataset wrangling, but data encryption is disabled in deployment\n",
      "2025-08-26 15:24:58,449:WARNING:thetaray.common.logging:Encryption is enabled on dataset party_wrangling, but data encryption is disabled in deployment\n",
      "2025-08-26 15:24:58,450:WARNING:thetaray.common.logging:Encryption is enabled on dataset customer_insights, but data encryption is disabled in deployment\n",
      "2025-08-26 15:24:58,451:WARNING:thetaray.common.logging:Encryption is enabled on dataset transaction, but data encryption is disabled in deployment\n",
      "2025-08-26 15:24:58,452:WARNING:thetaray.common.logging:Encryption is enabled on dataset account, but data encryption is disabled in deployment\n",
      "2025-08-26 15:24:58,458:WARNING:thetaray.common.logging:Encryption is enabled on dataset demo_nested_banking_customer_insights, but data encryption is disabled in deployment\n",
      "2025-08-26 15:24:58,470:WARNING:thetaray.common.logging:Encryption is enabled on dataset demo_pay_proc_customer_insights, but data encryption is disabled in deployment\n",
      "2025-08-26 15:24:58,495:WARNING:thetaray.common.logging:Encryption is enabled on dataset demo_digital_wallets_customer_insights, but data encryption is disabled in deployment\n",
      "2025-08-26 15:24:58,512:WARNING:thetaray.common.logging:Encryption is enabled on dataset demo_ret_indiv_customer_insights, but data encryption is disabled in deployment\n",
      "2025-08-26 15:24:58,537:WARNING:thetaray.common.logging:Encryption is enabled on dataset demo_merchant_customer_insights, but data encryption is disabled in deployment\n",
      "2025-08-26 15:24:58,685:WARNING:thetaray.common.logging:Property AN is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,686:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,686:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,686:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,686:WARNING:thetaray.common.logging:Node AC is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,686:WARNING:thetaray.common.logging:Property PI is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,686:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,686:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,686:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,686:WARNING:thetaray.common.logging:Node PR is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,686:WARNING:thetaray.common.logging:Edge TX is missing is_monetary property. Defaulting to True\n",
      "2025-08-26 15:24:58,686:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,686:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Property AN is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Node AC is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Property PI is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Node PR is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Edge TX is missing is_monetary property. Defaulting to True\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,687:WARNING:thetaray.common.logging:Property AN is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,688:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,688:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,688:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,688:WARNING:thetaray.common.logging:Node AC is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,688:WARNING:thetaray.common.logging:Property PI is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,688:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,688:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,688:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,688:WARNING:thetaray.common.logging:Node PR is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,688:WARNING:thetaray.common.logging:Edge TX is missing is_monetary property. Defaulting to True\n",
      "2025-08-26 15:24:58,688:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,688:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,688:WARNING:thetaray.common.logging:Property AN is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,689:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,689:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,689:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,689:WARNING:thetaray.common.logging:Node AC is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,689:WARNING:thetaray.common.logging:Property PI is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,689:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,689:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,689:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,689:WARNING:thetaray.common.logging:Node PR is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,689:WARNING:thetaray.common.logging:Edge TX is missing is_monetary property. Defaulting to True\n",
      "2025-08-26 15:24:58,689:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,689:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,759:WARNING:thetaray.common.logging:Property AN is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,759:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,759:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,759:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,759:WARNING:thetaray.common.logging:Node AC is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,759:WARNING:thetaray.common.logging:Property PI is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Node PR is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Edge TX is missing is_monetary property. Defaulting to True\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Property AN is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Node AC is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Property PI is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,760:WARNING:thetaray.common.logging:Node PR is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,761:WARNING:thetaray.common.logging:Edge TX is missing is_monetary property. Defaulting to True\n",
      "2025-08-26 15:24:58,761:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,761:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,764:WARNING:thetaray.common.logging:Property AN is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,764:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,764:WARNING:thetaray.common.logging:Property CN is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,764:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,764:WARNING:thetaray.common.logging:Node AC is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,764:WARNING:thetaray.common.logging:Property PI is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,764:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,764:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,764:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,764:WARNING:thetaray.common.logging:Node PR is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,764:WARNING:thetaray.common.logging:Edge TX is missing is_monetary property. Defaulting to True\n",
      "2025-08-26 15:24:58,765:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,765:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,769:WARNING:thetaray.common.logging:Property AN is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,769:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,769:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,769:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,769:WARNING:thetaray.common.logging:Node AC is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,769:WARNING:thetaray.common.logging:Property PI is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,769:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,769:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,769:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,769:WARNING:thetaray.common.logging:Node PR is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,769:WARNING:thetaray.common.logging:Edge TX is missing is_monetary property. Defaulting to True\n",
      "2025-08-26 15:24:58,769:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,769:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,770:WARNING:thetaray.common.logging:Property AN is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,770:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,770:WARNING:thetaray.common.logging:Property CN is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,770:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,770:WARNING:thetaray.common.logging:Node AC is configured as encrypted, but data encryption is disabled\n",
      "2025-08-26 15:24:58,770:WARNING:thetaray.common.logging:Edge TX is missing is_monetary property. Defaulting to True\n",
      "2025-08-26 15:24:58,829:WARNING:thetaray.common.logging:Parameter must be unique within the evaluation flow [demo_ret_smb_ef][{'tr_rb_param_1': ['high_transfer_jurisd', 'high_cash_withdrawal']}]\n",
      "2025-08-26 15:24:58,853:ERROR:thetaray.common.logging:failed to load solution\n",
      "Traceback (most recent call last):\n",
      "  File \"/thetaray/platform/python/thetaray/api/solution/loader.py\", line 100, in _load_solution\n",
      "    smd_dict['risk_manager'] = load_risk(smd_dict['datasets'], smd_dict['evaluation_flows'],smd_dict['enrichments_manager'], user_enrichments_manager, Settings.DOMAINS_PATH, disabled_domains)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/thetaray/platform/python/thetaray/api/solution/risks_manager.py\", line 831, in load_risk\n",
      "    risk_manager = RisksManager(\n",
      "                   ^^^^^^^^^^^^^\n",
      "  File \"/thetaray/platform/python/thetaray/api/solution/risks_manager.py\", line 77, in __init__\n",
      "    self._load_risks()\n",
      "  File \"/thetaray/platform/python/thetaray/api/solution/risks_manager.py\", line 534, in _load_risks\n",
      "    validate_risks(\n",
      "  File \"/thetaray/platform/python/thetaray/api/solution/risk_validator.py\", line 51, in validate_risks\n",
      "    _validate_conditions(risk_manager, enrichments_manager, risks, datasets, evaluation_flows)\n",
      "  File \"/thetaray/platform/python/thetaray/api/solution/risk_validator.py\", line 244, in _validate_conditions\n",
      "    field_names = _get_field_names(\n",
      "                  ^^^^^^^^^^^^^^^^^\n",
      "  File \"/thetaray/platform/python/thetaray/api/solution/risk_validator.py\", line 177, in _get_field_names\n",
      "    input_ds: DataSet = get_ds_metadata_by_identifier_from_list(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/thetaray/platform/python/thetaray/api/solution/metadata_functions.py\", line 30, in get_ds_metadata_by_identifier_from_list\n",
      "    raise ValueError(\"Metadata for dataset: \" + identifier + \" does not exist\")\n",
      "ValueError: Metadata for dataset: demo_nested_banking_customer_monthly does not exist\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"/thetaray/platform/python/thetaray/api/solution/loader.py\", line 100, in _load_solution\n    smd_dict['risk_manager'] = load_risk(smd_dict['datasets'], smd_dict['evaluation_flows'],smd_dict['enrichments_manager'], user_enrichments_manager, Settings.DOMAINS_PATH, disabled_domains)\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/thetaray/platform/python/thetaray/api/solution/risks_manager.py\", line 831, in load_risk\n    risk_manager = RisksManager(\n                   ^^^^^^^^^^^^^\n  File \"/thetaray/platform/python/thetaray/api/solution/risks_manager.py\", line 77, in __init__\n    self._load_risks()\n  File \"/thetaray/platform/python/thetaray/api/solution/risks_manager.py\", line 534, in _load_risks\n    validate_risks(\n  File \"/thetaray/platform/python/thetaray/api/solution/risk_validator.py\", line 51, in validate_risks\n    _validate_conditions(risk_manager, enrichments_manager, risks, datasets, evaluation_flows)\n  File \"/thetaray/platform/python/thetaray/api/solution/risk_validator.py\", line 244, in _validate_conditions\n    field_names = _get_field_names(\n                  ^^^^^^^^^^^^^^^^^\n  File \"/thetaray/platform/python/thetaray/api/solution/risk_validator.py\", line 177, in _get_field_names\n    input_ds: DataSet = get_ds_metadata_by_identifier_from_list(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/thetaray/platform/python/thetaray/api/solution/metadata_functions.py\", line 30, in get_ds_metadata_by_identifier_from_list\n    raise ValueError(\"Metadata for dataset: \" + identifier + \" does not exist\")\nValueError: Metadata for dataset: demo_nested_banking_customer_monthly does not exist\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_240/1533840305.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Inicializar contexto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m context = init_context(\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mexecution_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1970\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mspark_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/thetaray/platform/python/thetaray/api/context/context.py\u001b[0m in \u001b[0;36minit_context\u001b[0;34m(job_name, execution_date, mlflow_run_id, spark_conf, spark_master, solution, domain, delete_unused_columns, allow_type_changes, drop_undefined_datasets, skip_jars)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msolution\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;31m# serialized_context will be injected as a global variable in case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/thetaray/platform/python/thetaray/api/solution/loader.py\u001b[0m in \u001b[0;36mload_solution\u001b[0;34m(context, load_risks, solution_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Solution loaded successfully, digest: {result.md5_digest()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"/thetaray/platform/python/thetaray/api/solution/loader.py\", line 100, in _load_solution\n    smd_dict['risk_manager'] = load_risk(smd_dict['datasets'], smd_dict['evaluation_flows'],smd_dict['enrichments_manager'], user_enrichments_manager, Settings.DOMAINS_PATH, disabled_domains)\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/thetaray/platform/python/thetaray/api/solution/risks_manager.py\", line 831, in load_risk\n    risk_manager = RisksManager(\n                   ^^^^^^^^^^^^^\n  File \"/thetaray/platform/python/thetaray/api/solution/risks_manager.py\", line 77, in __init__\n    self._load_risks()\n  File \"/thetaray/platform/python/thetaray/api/solution/risks_manager.py\", line 534, in _load_risks\n    validate_risks(\n  File \"/thetaray/platform/python/thetaray/api/solution/risk_validator.py\", line 51, in validate_risks\n    _validate_conditions(risk_manager, enrichments_manager, risks, datasets, evaluation_flows)\n  File \"/thetaray/platform/python/thetaray/api/solution/risk_validator.py\", line 244, in _validate_conditions\n    field_names = _get_field_names(\n                  ^^^^^^^^^^^^^^^^^\n  File \"/thetaray/platform/python/thetaray/api/solution/risk_validator.py\", line 177, in _get_field_names\n    input_ds: DataSet = get_ds_metadata_by_identifier_from_list(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/thetaray/platform/python/thetaray/api/solution/metadata_functions.py\", line 30, in get_ds_metadata_by_identifier_from_list\n    raise ValueError(\"Metadata for dataset: \" + identifier + \" does not exist\")\nValueError: Metadata for dataset: demo_nested_banking_customer_monthly does not exist\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import uuid\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from faker import Faker\n",
    "from thetaray.api.context import init_context\n",
    "from pyspark.sql import functions as f\n",
    "from thetaray.common.data_environment import DataEnvironment\n",
    "\n",
    "\n",
    "# Configuración logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(message)s')\n",
    "\n",
    "# Configuración pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Cargar configuración Spark\n",
    "with open('/thetaray/git/solutions/domains/demo_merchant/config/spark_config.yaml') as spark_config_file:\n",
    "    spark_config = yaml.load(spark_config_file, yaml.FullLoader)['spark_config_a']\n",
    "\n",
    "\n",
    "# Inicializar contexto\n",
    "context = init_context(\n",
    "    execution_date=datetime(1970, 2, 1),\n",
    "    spark_conf=spark_config,\n",
    "    # spark_master='local[*]', # drop\n",
    "    allow_type_changes=True,\n",
    "    drop_undefined_datasets=True,\n",
    "    delete_unused_columns=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef5457-74fe-4ca7-b2a6-4e5e6f256bec",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459ed148-824f-406b-9bb3-9008880ea8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thetaray.api.dataset import dataset_functions\n",
    "\n",
    "from domains.demo_merchant.datasets.transactions import transactions_dataset\n",
    "from domains.demo_merchant.datasets.customer_monthly import customer_monthly_dataset\n",
    "# from domains.demo_merchant.datasets.customer_insights import customer_insights_dataset \n",
    "from domains.demo_merchant.datasets.customers import customers_dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb3220-a8b0-4b58-9649-6ee9a698f920",
   "metadata": {},
   "source": [
    "# Data Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2079b577-d4eb-40cb-bef5-e1d93168c070",
   "metadata": {},
   "source": [
    "### 1. Transactions and Aggregate Features Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f1687-fc30-4008-b467-4ad50ecfb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from itertools import count\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "\n",
    "# -------------------- US market basics --------------------\n",
    "US_STATES = [\n",
    "    \"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DE\",\"FL\",\"GA\",\"HI\",\"ID\",\"IL\",\"IN\",\"IA\",\"KS\",\"KY\",\"LA\",\"ME\",\"MD\",\n",
    "    \"MA\",\"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\"ND\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\n",
    "    \"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\"WV\",\"WI\",\"WY\",\"DC\",\"PR\"\n",
    "]\n",
    "\n",
    "MCC_LIST = [\n",
    "    (\"5411\",\"Grocery Stores, Supermarkets\"),\n",
    "    (\"5812\",\"Eating Places, Restaurants\"),\n",
    "    (\"5814\",\"Fast Food Restaurants\"),\n",
    "    (\"4111\",\"Local Passenger Transportation\"),\n",
    "    (\"5732\",\"Electronics Stores\"),\n",
    "    (\"5691\",\"Clothing Stores\"),\n",
    "    (\"5942\",\"Book Stores\"),\n",
    "    (\"4814\",\"Telecommunication Services\"),\n",
    "    (\"5977\",\"Cosmetic Stores\"),\n",
    "    (\"5999\",\"Specialty Retail Stores\"),\n",
    "]\n",
    "\n",
    "CARD_BRANDS = [\"Visa\",\"Mastercard\",\"Amex\",\"Discover\"]\n",
    "CHANNELS = [\"card_present\",\"ecommerce\"]\n",
    "BUSINESS_NAMES = [\n",
    "    \"River Market\",\"Sunset Electronics\",\"Liberty Diner\",\"Bluebird Books\",\"Metro Rides\",\n",
    "    \"Pioneer Outfitters\",\"Cedar Grocery\",\"North Star Grill\",\"Harbor Cafe\",\"Prairie Style\"\n",
    "]\n",
    "\n",
    "# -------------------- helpers --------------------\n",
    "def _month_starts(end_date: datetime, months_total: int) -> List[datetime]:\n",
    "    anchor = datetime(end_date.year, end_date.month, 1)\n",
    "    return [(anchor - pd.DateOffset(months=m)).to_pydatetime()\n",
    "            for m in range(months_total - 1, -1, -1)]\n",
    "\n",
    "def _pick_active_months(months_total: int, active_months: int) -> List[int]:\n",
    "    m = min(active_months, months_total)\n",
    "    return sorted(random.sample(range(months_total), m))\n",
    "\n",
    "def _next_id(prefix: str, counter: count) -> str:\n",
    "    return f\"{prefix}{next(counter):010d}\"\n",
    "\n",
    "def _month_key(dt: datetime) -> str:\n",
    "    return dt.strftime(\"%Y-%m\")\n",
    "\n",
    "def _detect_rapid_pattern(df_month: pd.DataFrame,\n",
    "                          small_amt_thresh: float,\n",
    "                          small_repeats: int,\n",
    "                          large_multiple: float,\n",
    "                          window_minutes: int = 240) -> float:\n",
    "    \"\"\"Return 1.0 if pattern exists, otherwise 0.0 (DOUBLE).\"\"\"\n",
    "    if df_month.empty:\n",
    "        return 0.0\n",
    "    d = df_month.sort_values(\"transaction_datetime\").copy()\n",
    "    d[\"date\"] = d[\"transaction_datetime\"].dt.date\n",
    "    for _, g in d.groupby(\"date\"):\n",
    "        g = g.sort_values(\"transaction_datetime\")\n",
    "        small_mask = g[\"amount\"] <= small_amt_thresh\n",
    "        small_times = g.loc[small_mask, \"transaction_datetime\"].tolist()\n",
    "        if len(small_times) >= small_repeats:\n",
    "            first_small = small_times[0]\n",
    "            window_end = first_small + timedelta(minutes=window_minutes)\n",
    "            in_window = g[g[\"transaction_datetime\"].between(first_small, window_end)]\n",
    "            if not in_window.empty:\n",
    "                large_thresh = small_amt_thresh * large_multiple\n",
    "                if (in_window[\"amount\"] >= large_thresh).any():\n",
    "                    return 1.0\n",
    "    return 0.0\n",
    "\n",
    "# -------------------- main generator --------------------\n",
    "def generate_ma_fake_transactions(\n",
    "    # --- scale & horizon ---\n",
    "    n_merchants: int = 200,\n",
    "    months_total: int = 12,\n",
    "    active_months_per_merchant: int = 12,\n",
    "    end_date: Optional[datetime] = None,\n",
    "    avg_txns_per_active_month: float = 800.0,\n",
    "    currency: str = \"USD\",\n",
    "    seed: Optional[int] = 42,\n",
    "\n",
    "    # --- TARGET: features that SHOULD NOT trigger (kept calm) ---\n",
    "    ensure_no_mismatch: bool = True,            # revenue_mismatch calm\n",
    "    ensure_no_avg_ticket_shift: bool = True,    # avg_txn_amt_ratio calm\n",
    "    ensure_no_rapid_pattern: bool = True,       # rapid_load_transfer calm\n",
    "\n",
    "    # --- knobs for calm features ---\n",
    "    declared_revenue_noise_std: float = 0.02,   # ~2% noise => low mismatch\n",
    "    avg_ticket_monthly_jitter: float = 0.05,    # ~5% drift between months\n",
    "    rapid_small_txn_amount: float = 4.99,\n",
    "    rapid_small_repeats: int = 12,\n",
    "    rapid_large_multiplier: float = 50.0,\n",
    "    rapid_window_minutes: int = 240,\n",
    "\n",
    "    # --- TARGET: features that SHOULD trigger (inject anomalies by default) ---\n",
    "    # Low value ratio\n",
    "    low_value_threshold: float = 5.00,\n",
    "    pct_merchants_low_value_anom: float = 0.15,     # fraction of merchants with at least one month high ratio\n",
    "    low_value_ratio_target: float = 0.40,           # target ratio in an anomalous month\n",
    "    # Dormant account\n",
    "    dormant_lookback_months: int = 3,\n",
    "    pct_merchants_dormant_anom: float = 0.12,       # merchants that wake up after dormancy\n",
    "    # Spike of Transactions\n",
    "    spike_baseline_window: int = 6,\n",
    "    spike_ratio_threshold: float = 1.8,\n",
    "    pct_merchants_spike_anom: float = 0.15,         # merchants with 1 spiky month\n",
    "    spike_volume_multiplier: float = 2.2,           # multiplies lambda in spike month\n",
    "    # Refund count ratio\n",
    "    refund_rate_base: float = 0.008,                # ~0.8% normal\n",
    "    chargeback_rate_base: float = 0.001,            # optional (not in feature)\n",
    "    pct_merchants_refund_anom: float = 0.14,        # merchants with high refund month\n",
    "    refund_rate_anom: float = 0.05,                 # 5% refunds in anomalous month\n",
    "\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        transactions_df: one row per transaction\n",
    "        mm: monthly features per merchant with EXACT columns (all DOUBLE):\n",
    "            merchant_id, year_month, low_value_trx_ratio, is_dormant_account,\n",
    "            spike_of_trx, refund_count_ratio, revenue_mismatch, avg_txn_amt_ratio,\n",
    "            rapid_load_transfer\n",
    "\n",
    "    Trigger policy implemented:\n",
    "      - TRIGGER (Yes): low_value_trx_ratio↑, is_dormant_account=1.0 in wake-up month,\n",
    "                       spike_of_trx=1.0 in spiky month, refund_count_ratio↑\n",
    "      - NO TRIGGER (No): revenue_mismatch≈low, avg_txn_amt_ratio≈1, rapid_load_transfer=0.0\n",
    "    \"\"\"\n",
    "    # seed\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed); random.seed(seed)\n",
    "    if end_date is None:\n",
    "        end_date = datetime.utcnow()\n",
    "\n",
    "    # months\n",
    "    month_starts = _month_starts(end_date, months_total)\n",
    "    month_ranges = [(ms, (ms + pd.DateOffset(months=1)).to_pydatetime() - timedelta(seconds=1))\n",
    "                    for ms in month_starts]\n",
    "\n",
    "    # merchants\n",
    "    m_ids = [f\"M{100000 + i}\" for i in range(n_merchants)]\n",
    "    m_names = {m: random.choice(BUSINESS_NAMES) + f\" #{i%97+1}\" for i, m in enumerate(m_ids)}\n",
    "    m_state = {m: random.choice(US_STATES) for m in m_ids}\n",
    "    m_mcc, m_mcc_desc = zip(*[random.choice(MCC_LIST) for _ in m_ids])\n",
    "    m_mcc = {m: m_mcc[i] for i, m in enumerate(m_ids)}\n",
    "    m_mcc_desc = {m: m_mcc_desc[i] for i, m in enumerate(m_ids)}\n",
    "\n",
    "    # per-merchant base avg ticket (by MCC)\n",
    "    m_base_ticket: Dict[str, float] = {}\n",
    "    for m in m_ids:\n",
    "        mcc = m_mcc[m]\n",
    "        if mcc in {\"5812\",\"5814\"}:       base = np.random.lognormal(np.log(18), 0.25)\n",
    "        elif mcc in {\"5411\"}:            base = np.random.lognormal(np.log(32), 0.25)\n",
    "        elif mcc in {\"5732\"}:            base = np.random.lognormal(np.log(85), 0.30)\n",
    "        elif mcc in {\"4111\"}:            base = np.random.lognormal(np.log(14), 0.22)\n",
    "        else:                             base = np.random.lognormal(np.log(28), 0.28)\n",
    "        m_base_ticket[m] = max(3.0, float(base))\n",
    "\n",
    "    # baseline txn volume per merchant\n",
    "    m_base_lambda = {m: max(30.0, np.random.normal(avg_txns_per_active_month, avg_txns_per_active_month*0.2))\n",
    "                     for m in m_ids}\n",
    "\n",
    "    # choose anomaly cohorts (for TRIGGER features)\n",
    "    def pick_subset(pct: float) -> set:\n",
    "        k = max(1, int(round(pct * n_merchants))) if n_merchants else 0\n",
    "        return set(random.sample(m_ids, min(k, n_merchants))) if k > 0 else set()\n",
    "\n",
    "    cohort_lowval   = pick_subset(pct_merchants_low_value_anom)\n",
    "    cohort_dormant  = pick_subset(pct_merchants_dormant_anom)\n",
    "    cohort_spike    = pick_subset(pct_merchants_spike_anom)\n",
    "    cohort_refund   = pick_subset(pct_merchants_refund_anom)\n",
    "\n",
    "    # For \"No trigger\" trio\n",
    "    rapid_merchants = set() if ensure_no_rapid_pattern else pick_subset(0.08)\n",
    "\n",
    "    # tracking\n",
    "    tid = count(1); cid = count(1)\n",
    "    rows = []\n",
    "    # pre-aggregation holders\n",
    "    obs_sales: Dict[Tuple[str, str], float] = {}\n",
    "    obs_count: Dict[Tuple[str, str], int] = {}\n",
    "    refunds_count: Dict[Tuple[str, str], int] = {}\n",
    "    low_value_count: Dict[Tuple[str, str], int] = {}\n",
    "    # month indices storage for rapid detector\n",
    "    per_month_txn_idx: Dict[Tuple[str, str], List[int]] = {}\n",
    "\n",
    "    # For dormant anomaly: force first N months zero, then active\n",
    "    dormant_cut_idx = max(dormant_lookback_months, 1)\n",
    "\n",
    "    # choose one anomalous month per cohort (for low value / spike / refund)\n",
    "    def choose_anom_month() -> int:\n",
    "        # avoid very first month to have some baseline\n",
    "        return random.randint(max(1, spike_baseline_window//2), months_total-1)\n",
    "\n",
    "    lowval_month_by_m  = {m: choose_anom_month() for m in cohort_lowval}\n",
    "    spike_month_by_m   = {m: choose_anom_month() for m in cohort_spike}\n",
    "    refund_month_by_m  = {m: choose_anom_month() for m in cohort_refund}\n",
    "\n",
    "    for m in m_ids:\n",
    "        base_ticket = m_base_ticket[m]\n",
    "        base_lambda = m_base_lambda[m]\n",
    "\n",
    "        # activity plan: if dormant cohort, zero tx for first dormant_cut_idx months then resume\n",
    "        active_idx = list(range(months_total))  # start with all months potentially active\n",
    "        if m in cohort_dormant:\n",
    "            # zero-out first dormant_cut_idx months; activity resumes afterwards\n",
    "            active_idx = list(range(dormant_cut_idx, months_total))\n",
    "\n",
    "        for idx, (m_start, m_end) in enumerate(month_ranges):\n",
    "            ym = _month_key(m_start)\n",
    "            month_txn_indices: List[int] = []\n",
    "\n",
    "            # decide activity\n",
    "            if idx not in active_idx:\n",
    "                per_month_txn_idx[(m, ym)] = month_txn_indices\n",
    "                continue  # no transactions\n",
    "\n",
    "            # monthly drift (calm vs. not)\n",
    "            if ensure_no_avg_ticket_shift:\n",
    "                ticket_mu = base_ticket * (1.0 + np.random.normal(0.0, avg_ticket_monthly_jitter))\n",
    "                lam = max(5.0, np.random.normal(base_lambda, base_lambda * 0.07))\n",
    "            else:\n",
    "                ticket_mu = base_ticket * (1.0 + np.random.normal(0.0, max(0.22, avg_ticket_monthly_jitter*3)))\n",
    "                lam = max(5.0, np.random.normal(base_lambda, base_lambda * 0.25))\n",
    "\n",
    "            # spike anomaly: boost lambda in the chosen month\n",
    "            if m in cohort_spike and idx == spike_month_by_m[m]:\n",
    "                lam *= spike_volume_multiplier\n",
    "\n",
    "            n_tx = int(max(1, np.random.poisson(lam)))\n",
    "\n",
    "            # refund rate (anomalous month?)\n",
    "            month_refund_rate = refund_rate_anom if (m in cohort_refund and idx == refund_month_by_m[m]) else refund_rate_base\n",
    "\n",
    "            # low-value anomaly: force a fraction of low-amount txns in that month\n",
    "            make_lowval_heavy = (m in cohort_lowval and idx == lowval_month_by_m[m])\n",
    "            target_lowval_ratio = low_value_ratio_target if make_lowval_heavy else None\n",
    "            lowval_needed = int(round(target_lowval_ratio * n_tx)) if target_lowval_ratio else 0\n",
    "\n",
    "            # generate transactions\n",
    "            lowvals_assigned = 0\n",
    "            for t in range(n_tx):\n",
    "                span_sec = int((m_end - m_start).total_seconds())\n",
    "                ts = m_start + timedelta(seconds=random.randint(0, span_sec))\n",
    "\n",
    "                # amount: assign explicit low-values first if anomaly month\n",
    "                if make_lowval_heavy and lowvals_assigned < lowval_needed:\n",
    "                    amount = round(np.random.uniform(0.5, low_value_threshold), 2)\n",
    "                    lowvals_assigned += 1\n",
    "                else:\n",
    "                    amount = float(np.random.lognormal(mean=np.log(ticket_mu), sigma=0.35))\n",
    "                    amount = round(max(0.5, amount), 2)\n",
    "\n",
    "                is_ref = (np.random.rand() < month_refund_rate)\n",
    "                is_cbk = (np.random.rand() < chargeback_rate_base)\n",
    "\n",
    "                rows.append({\n",
    "                    \"transaction_id\": _next_id(\"T\", tid),\n",
    "                    \"merchant_id\": m,\n",
    "                    \"merchant_name\": m_names[m],\n",
    "                    \"mcc\": m_mcc[m],\n",
    "                    \"mcc_description\": m_mcc_desc[m],\n",
    "                    \"state\": m_state[m],\n",
    "                    \"transaction_datetime\": ts,\n",
    "                    \"amount\": amount,\n",
    "                    \"currency\": currency,\n",
    "                    \"channel\": random.choice(CHANNELS),\n",
    "                    \"card_brand\": random.choice(CARD_BRANDS),\n",
    "                    \"is_refund\": is_ref,\n",
    "                    \"is_chargeback\": is_cbk,\n",
    "                    \"customer_id\": f\"C{next(cid):09d}\"\n",
    "                })\n",
    "                month_txn_indices.append(len(rows)-1)\n",
    "\n",
    "                key = (m, ym)\n",
    "                obs_sales[key] = obs_sales.get(key, 0.0) + amount\n",
    "                obs_count[key] = obs_count.get(key, 0) + 1\n",
    "                refunds_count[key] = refunds_count.get(key, 0) + (1 if is_ref else 0)\n",
    "                low_value_count[key] = low_value_count.get(key, 0) + (1 if amount <= low_value_threshold else 0)\n",
    "\n",
    "            # optional: (NOT trigger by default) rapid pattern\n",
    "            if m in rapid_merchants:\n",
    "                base_day = random.randint(3, 25)\n",
    "                # many small\n",
    "                for i in range(rapid_small_repeats):\n",
    "                    ts = m_start + timedelta(days=min(base_day, 27), hours=9, minutes=min(59, 2*i))\n",
    "                    amt = rapid_small_txn_amount\n",
    "                    rows.append({\n",
    "                        \"transaction_id\": _next_id(\"T\", tid),\n",
    "                        \"merchant_id\": m,\n",
    "                        \"merchant_name\": m_names[m],\n",
    "                        \"mcc\": m_mcc[m],\n",
    "                        \"mcc_description\": m_mcc_desc[m],\n",
    "                        \"state\": m_state[m],\n",
    "                        \"transaction_datetime\": ts,\n",
    "                        \"amount\": amt,\n",
    "                        \"currency\": currency,\n",
    "                        \"channel\": \"ecommerce\",\n",
    "                        \"card_brand\": random.choice(CARD_BRANDS),\n",
    "                        \"is_refund\": False,\n",
    "                        \"is_chargeback\": False,\n",
    "                        \"customer_id\": f\"C{next(cid):09d}\"\n",
    "                    })\n",
    "                    month_txn_indices.append(len(rows)-1)\n",
    "                    key = (m, ym)\n",
    "                    obs_sales[key] = obs_sales.get(key, 0.0) + amt\n",
    "                    obs_count[key] = obs_count.get(key, 0) + 1\n",
    "                    low_value_count[key] = low_value_count.get(key, 0) + (1 if amt <= low_value_threshold else 0)\n",
    "\n",
    "                # one big\n",
    "                ts_big = m_start + timedelta(days=min(base_day, 27), hours=12, minutes=0)\n",
    "                big_amt = round(rapid_small_txn_amount * rapid_large_multiplier, 2)\n",
    "                rows.append({\n",
    "                    \"transaction_id\": _next_id(\"T\", tid),\n",
    "                    \"merchant_id\": m,\n",
    "                    \"merchant_name\": m_names[m],\n",
    "                    \"mcc\": m_mcc[m],\n",
    "                    \"mcc_description\": m_mcc_desc[m],\n",
    "                    \"state\": m_state[m],\n",
    "                    \"transaction_datetime\": ts_big,\n",
    "                    \"amount\": big_amt,\n",
    "                    \"currency\": currency,\n",
    "                    \"channel\": \"ecommerce\",\n",
    "                    \"card_brand\": random.choice(CARD_BRANDS),\n",
    "                    \"is_refund\": False,\n",
    "                    \"is_chargeback\": False,\n",
    "                    \"customer_id\": f\"C{next(cid):09d}\"\n",
    "                })\n",
    "                month_txn_indices.append(len(rows)-1)\n",
    "                key = (m, ym)\n",
    "                obs_sales[key] = obs_sales.get(key, 0.0) + big_amt\n",
    "                obs_count[key] = obs_count.get(key, 0) + 1\n",
    "                # big amount not low-value\n",
    "\n",
    "            per_month_txn_idx[(m, ym)] = month_txn_indices\n",
    "\n",
    "    # transactions df\n",
    "    transactions_df = pd.DataFrame(rows)\n",
    "    if not transactions_df.empty:\n",
    "        transactions_df[\"transaction_datetime\"] = pd.to_datetime(transactions_df[\"transaction_datetime\"])\n",
    "        transactions_df[\"amount\"] = transactions_df[\"amount\"].astype(float)\n",
    "        transactions_df[\"is_refund\"] = transactions_df[\"is_refund\"].astype(bool)\n",
    "        transactions_df[\"is_chargeback\"] = transactions_df[\"is_chargeback\"].astype(bool)\n",
    "        transactions_df = transactions_df.sort_values(\"transaction_datetime\").reset_index(drop=True)\n",
    "\n",
    "    # create full key grid merchant x month\n",
    "    year_month = [_month_key(ms) for ms in month_starts]\n",
    "    key_grid = pd.MultiIndex.from_product([m_ids, year_month], names=[\"merchant_id\",\"year_month\"]).to_frame(index=False)\n",
    "\n",
    "    # observed aggregates\n",
    "    def _get(obs_dict, k, default): return obs_dict.get(k, default)\n",
    "\n",
    "    key_grid[\"observed_sales\"] = key_grid.apply(lambda r: round(_get(obs_sales, (r[\"merchant_id\"], r[\"year_month\"]), 0.0), 2), axis=1)\n",
    "    key_grid[\"txn_count\"]      = key_grid.apply(lambda r: int(_get(obs_count, (r[\"merchant_id\"], r[\"year_month\"]), 0)), axis=1)\n",
    "    key_grid[\"avg_ticket\"]     = (key_grid[\"observed_sales\"] / key_grid[\"txn_count\"].replace(0, np.nan)).fillna(0.0).round(2)\n",
    "    key_grid[\"refunds\"]        = key_grid.apply(lambda r: int(_get(refunds_count, (r[\"merchant_id\"], r[\"year_month\"]), 0)), axis=1)\n",
    "    key_grid[\"low_value_cnt\"]  = key_grid.apply(lambda r: int(_get(low_value_count, (r[\"merchant_id\"], r[\"year_month\"]), 0)), axis=1)\n",
    "\n",
    "    # declared revenue (calm vs not)\n",
    "    if ensure_no_mismatch:\n",
    "        noise = np.random.normal(0.0, declared_revenue_noise_std, size=len(key_grid))\n",
    "    else:\n",
    "        noise = np.random.normal(0.0, max(0.12, declared_revenue_noise_std*6), size=len(key_grid))\n",
    "    key_grid[\"declared_revenue\"] = (key_grid[\"observed_sales\"] * (1.0 + noise)).round(2)\n",
    "\n",
    "    # ---------- FEATURES (ALL DOUBLE) ----------\n",
    "    mm = key_grid[[\"merchant_id\",\"year_month\",\"observed_sales\",\"txn_count\",\"avg_ticket\",\"declared_revenue\",\"refunds\",\"low_value_cnt\"]].copy()\n",
    "\n",
    "    # 1) low_value_trx_ratio (DOUBLE)\n",
    "    mm[\"low_value_trx_ratio\"] = (mm[\"low_value_cnt\"] / mm[\"txn_count\"].replace(0, np.nan)).fillna(0.0).astype(float)\n",
    "\n",
    "    # 2) is_dormant_account (DOUBLE: 1.0 if resumes after N zero months, else 0.0)\n",
    "    def _dormant_flag(g: pd.DataFrame) -> pd.Series:\n",
    "        counts = g[\"txn_count\"].values\n",
    "        out = np.zeros(len(counts), dtype=float)\n",
    "        for i in range(len(counts)):\n",
    "            if counts[i] > 0 and i >= dormant_lookback_months:\n",
    "                prev = counts[i-dormant_lookback_months:i]\n",
    "                if np.all(prev == 0):\n",
    "                    out[i] = 1.0\n",
    "        return pd.Series(out, index=g.index)\n",
    "    mm[\"is_dormant_account\"] = mm.groupby(\"merchant_id\", group_keys=False).apply(_dormant_flag).astype(float)\n",
    "\n",
    "    # 3) spike_of_trx (DOUBLE: 1.0 abnormal, else 0.0)\n",
    "    def _spike_flags(g: pd.DataFrame) -> pd.Series:\n",
    "        vals = g[\"txn_count\"].astype(float).values\n",
    "        out = np.zeros(len(vals), dtype=float)\n",
    "        for i in range(len(vals)):\n",
    "            start = max(0, i - spike_baseline_window)\n",
    "            baseline = vals[start:i]\n",
    "            mean_base = baseline.mean() if baseline.size > 0 else 0.0\n",
    "            if mean_base > 1.0 and vals[i] > mean_base * spike_ratio_threshold:\n",
    "                out[i] = 1.0\n",
    "        return pd.Series(out, index=g.index)\n",
    "    mm[\"spike_of_trx\"] = mm.groupby(\"merchant_id\", group_keys=False).apply(_spike_flags).astype(float)\n",
    "\n",
    "    # 4) refund_count_ratio (DOUBLE)\n",
    "    mm[\"refund_count_ratio\"] = (mm[\"refunds\"] / mm[\"txn_count\"].replace(0, np.nan)).fillna(0.0).astype(float)\n",
    "\n",
    "    # 5) revenue_mismatch (DOUBLE)  -> keep calm by default\n",
    "    mm[\"revenue_mismatch\"] = (np.abs(mm[\"declared_revenue\"] - mm[\"observed_sales\"]) /\n",
    "                              mm[\"observed_sales\"].replace(0, np.nan)).fillna(0.0).astype(float)\n",
    "\n",
    "    # 6) avg_txn_amt_ratio (DOUBLE) vs trailing median -> ~1.0 when calm\n",
    "    def _avg_ratio(g: pd.DataFrame) -> pd.Series:\n",
    "        vals = g[\"avg_ticket\"].astype(float).values\n",
    "        out = np.ones(len(vals), dtype=float)\n",
    "        for i in range(len(vals)):\n",
    "            base = vals[max(0, i - 6):i]  # trailing 6 by default\n",
    "            med = np.median(base) if base.size > 0 else 0.0\n",
    "            out[i] = (vals[i] / med) if med > 0 else 1.0\n",
    "        return pd.Series(out, index=g.index)\n",
    "    mm[\"avg_txn_amt_ratio\"] = mm.groupby(\"merchant_id\", group_keys=False).apply(_avg_ratio).astype(float)\n",
    "\n",
    "    # 7) rapid_load_transfer (DOUBLE: 1.0/0.0) -> keep 0.0 by default\n",
    "    def _rapid_for_row(r):\n",
    "        m, ym = r[\"merchant_id\"], r[\"year_month\"]\n",
    "        idx_list = per_month_txn_idx.get((m, ym), [])\n",
    "        if not idx_list:\n",
    "            return 0.0\n",
    "        month_df = transactions_df.loc[idx_list, [\"transaction_datetime\",\"amount\"]].copy()\n",
    "        return float(_detect_rapid_pattern(month_df,\n",
    "                                           small_amt_thresh=rapid_small_txn_amount,\n",
    "                                           small_repeats=rapid_small_repeats,\n",
    "                                           large_multiple=rapid_large_multiplier,\n",
    "                                           window_minutes=rapid_window_minutes))\n",
    "    mm[\"rapid_load_transfer\"] = mm.apply(_rapid_for_row, axis=1).astype(float)\n",
    "    \n",
    "    mm[\"year_month\"] = pd.to_datetime(mm[\"year_month\"], format=\"%Y-%m\")\n",
    "    mm[\"year_month_str\"] = mm[\"year_month\"].dt.strftime(\"%Y-%m\")\n",
    "    # final cast/order (all features DOUBLE)\n",
    "    mm = mm[[\n",
    "        \"merchant_id\",\"year_month_str\",\"year_month\",\n",
    "        \"low_value_trx_ratio\",\n",
    "        \"is_dormant_account\",\n",
    "        \"spike_of_trx\",\n",
    "        \"refund_count_ratio\",\n",
    "        \"revenue_mismatch\",\n",
    "        \"avg_txn_amt_ratio\",\n",
    "        \"rapid_load_transfer\"\n",
    "    ]].sort_values([\"merchant_id\",\"year_month\"]).reset_index(drop=True)\n",
    "\n",
    "    # ensure float dtype\n",
    "    for c in [\"low_value_trx_ratio\",\"is_dormant_account\",\"spike_of_trx\",\n",
    "              \"refund_count_ratio\",\"revenue_mismatch\",\"avg_txn_amt_ratio\",\"rapid_load_transfer\"]:\n",
    "        mm[c] = mm[c].astype(float)\n",
    "\n",
    "    return transactions_df, mm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e89e0-e56a-4787-a6b5-f69a949c7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)  150 merchants, 18 months\n",
    "df_trx, agg_df = generate_ma_fake_transactions(\n",
    "    n_merchants=150,\n",
    "    months_total=18,\n",
    "    active_months_per_merchant=12, \n",
    "    end_date=datetime(2025, 6, 30),\n",
    "    avg_txns_per_active_month=35, \n",
    "    currency=\"USD\",\n",
    "    ensure_no_mismatch=True,\n",
    "    ensure_no_avg_ticket_shift=True,\n",
    "    ensure_no_rapid_pattern=True,\n",
    "\n",
    "     # low value\n",
    "    low_value_threshold=9.0,\n",
    "    pct_merchants_low_value_anom=0.30,\n",
    "    low_value_ratio_target=0.50,\n",
    "\n",
    "    # dormant\n",
    "    dormant_lookback_months=3,\n",
    "    pct_merchants_dormant_anom=0.25,\n",
    "\n",
    "    # spike\n",
    "    spike_baseline_window=4,\n",
    "    spike_ratio_threshold=1.6,\n",
    "    pct_merchants_spike_anom=0.30,\n",
    "    spike_volume_multiplier=2.6,\n",
    "\n",
    "    # refunds\n",
    "    refund_rate_base=0.008,\n",
    "    chargeback_rate_base=0.001,\n",
    "    pct_merchants_refund_anom=0.30,\n",
    "    refund_rate_anom=0.075\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8320f7-8ac5-4d25-9c09-1444d01fe5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "agg_df[\"revenue_mismatch\"] = np.random.normal(loc=1.0, scale=0.02, size=len(agg_df))\n",
    "agg_df[\"avg_txn_amt_ratio\"] = np.random.normal(loc=0.95, scale=0.015, size=len(agg_df))\n",
    "agg_df[\"rapid_load_transfer\"] = np.random.normal(loc=0.05, scale=0.01, size=len(agg_df))\n",
    "\n",
    "# Asegurar que no haya valores extremos (recorte a un rango razonable)\n",
    "agg_df[\"revenue_mismatch\"] = agg_df[\"revenue_mismatch\"].clip(0.9, 1.1)\n",
    "agg_df[\"avg_txn_amt_ratio\"] = agg_df[\"avg_txn_amt_ratio\"].clip(0.9, 1.05)\n",
    "agg_df[\"rapid_load_transfer\"] = agg_df[\"rapid_load_transfer\"].clip(0.0, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3f92d-0a64-49a5-8280-56c3936f95cc",
   "metadata": {},
   "source": [
    "### 2. Anomalous KYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb091d55-e62b-4136-be2a-8dc6dab60349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from datetime import date, timedelta\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "# ------------------ small US catalogs ------------------\n",
    "US_STATES = [\"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DE\",\"FL\",\"GA\",\"HI\",\"ID\",\"IL\",\"IN\",\"IA\",\"KS\",\"KY\",\"LA\",\"ME\",\"MD\",\n",
    "             \"MA\",\"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\"ND\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\n",
    "             \"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\"WV\",\"WI\",\"WY\",\"DC\",\"PR\"]\n",
    "\n",
    "US_CITIES = [\n",
    "    (\"New York\",\"NY\"),(\"Los Angeles\",\"CA\"),(\"Chicago\",\"IL\"),(\"Houston\",\"TX\"),(\"Phoenix\",\"AZ\"),\n",
    "    (\"Philadelphia\",\"PA\"),(\"San Antonio\",\"TX\"),(\"San Diego\",\"CA\"),(\"Dallas\",\"TX\"),(\"San Jose\",\"CA\"),\n",
    "    (\"Austin\",\"TX\"),(\"Jacksonville\",\"FL\"),(\"Fort Worth\",\"TX\"),(\"Columbus\",\"OH\"),(\"Charlotte\",\"NC\")\n",
    "]\n",
    "\n",
    "MCC_LIST = [\n",
    "    (\"5411\",\"Grocery Stores, Supermarkets\"),\n",
    "    (\"5812\",\"Eating Places, Restaurants\"),\n",
    "    (\"5814\",\"Fast Food Restaurants\"),\n",
    "    (\"4111\",\"Local Passenger Transportation\"),\n",
    "    (\"5732\",\"Electronics Stores\"),\n",
    "    (\"5691\",\"Clothing Stores\"),\n",
    "    (\"5942\",\"Book Stores\"),\n",
    "    (\"4814\",\"Telecommunication Services\"),\n",
    "    (\"5977\",\"Cosmetic Stores\"),\n",
    "    (\"5999\",\"Specialty Retail Stores\"),\n",
    "]\n",
    "\n",
    "BUSINESS_WORDS = [\"River\",\"Sunset\",\"Liberty\",\"Bluebird\",\"Metro\",\"Pioneer\",\"Cedar\",\"North Star\",\"Harbor\",\"Prairie\"]\n",
    "BUSINESS_SUFFIX = [\"LLC\",\"Inc.\",\"Corp.\",\"Ltd.\",\"Company\",\"Group\",\"Holdings\"]\n",
    "\n",
    "HIGH_RISK_INDUSTRIES = {\"5944\",\"5967\",\"5993\",\"7273\",\"7995\",\"6051\",\"6211\"}  # (ejemplos típicos de alto riesgo)\n",
    "# Nota: si tu demo usa sólo MCC_LIST de arriba, marcamos high-risk mediante un flag independiente.\n",
    "\n",
    "# ------------------ helpers ------------------\n",
    "def _rand_business_name(seed_str: str) -> str:\n",
    "    base = random.choice(BUSINESS_WORDS)\n",
    "    suf  = random.choice(BUSINESS_SUFFIX)\n",
    "    return f\"{base} {seed_str} {suf}\"\n",
    "\n",
    "def _rand_phone() -> str:\n",
    "    return f\"+1{random.randint(200,999)}{random.randint(200,999)}{random.randint(1000,9999)}\"\n",
    "\n",
    "def _rand_email(bname: str) -> str:\n",
    "    clean = re.sub(r\"[^a-z0-9]+\",\"\", bname.lower())\n",
    "    return f\"contact@{clean[:12] or 'merchant'}.com\"\n",
    "\n",
    "def _rand_website(bname: str) -> str:\n",
    "    clean = re.sub(r\"[^a-z0-9]+\",\"\", bname.lower())\n",
    "    return f\"https://www.{clean[:15] or 'merchant'}.com\"\n",
    "\n",
    "def _rand_postal_code() -> str:\n",
    "    return f\"{random.randint(10000, 99999)}\"\n",
    "\n",
    "def _rand_address() -> tuple[str,str,str,str]:\n",
    "    street_no = random.randint(10, 9999)\n",
    "    street_nm = random.choice([\"Main St\",\"Market St\",\"Broadway\",\"1st Ave\",\"2nd Ave\",\"Park Ave\",\"Oak St\",\"Pine St\"])\n",
    "    city, st = random.choice(US_CITIES)\n",
    "    return (f\"{street_no} {street_nm}\", city, st, _rand_postal_code())\n",
    "\n",
    "def _rand_ein(correct: bool = True) -> str:\n",
    "    if correct:\n",
    "        # EIN format: NN-NNNNNNN\n",
    "        return f\"{random.randint(10,99)}-{random.randint(1000000,9999999)}\"\n",
    "    # incorrect patterns to simulate anomaly\n",
    "    choices = [\n",
    "        f\"{random.randint(1,9)}{random.randint(0,9)}{random.randint(1000000,9999999)}\",    # missing hyphen\n",
    "        f\"{random.randint(100,999)}-{random.randint(100000,999999)}\",                      # wrong length\n",
    "        f\"{random.randint(10,99)}-{random.randint(10000,99999)}{random.choice(['A','X'])}\" # non-digit\n",
    "    ]\n",
    "    return random.choice(choices)\n",
    "\n",
    "def _choose_mcc(meta_row: Optional[pd.Series]) -> tuple[str,str]:\n",
    "    if meta_row is not None and \"mcc\" in meta_row and pd.notnull(meta_row[\"mcc\"]):\n",
    "        desc = meta_row.get(\"mcc_description\", \"\")\n",
    "        if not desc:\n",
    "            for code, d in MCC_LIST:\n",
    "                if code == str(meta_row[\"mcc\"]): desc = d\n",
    "        return str(meta_row[\"mcc\"]), (desc or \"NA\")\n",
    "    code, desc = random.choice(MCC_LIST)\n",
    "    return code, desc\n",
    "\n",
    "# ------------------ main ------------------\n",
    "def generate_merchant_kyc(\n",
    "    merchant_ids: Optional[Iterable[str]] = None,\n",
    "    transactions_df: Optional[pd.DataFrame] = None,  # alt: derive ids & meta (merchant_name/mcc/state)\n",
    "    merchant_meta: Optional[pd.DataFrame] = None,    # optional columns: merchant_id, merchant_name, mcc, mcc_description, state\n",
    "    seed: int = 42,\n",
    "\n",
    "    # anomaly knobs (prevalence)\n",
    "    pct_sanctioned_hit: float = 0.01,\n",
    "    pct_pep_owner: float = 0.02,\n",
    "    pct_adverse_media: float = 0.06,\n",
    "    pct_missing_docs: float = 0.10,\n",
    "    pct_incorrect_tax_id: float = 0.05,\n",
    "    pct_po_box_address: float = 0.05,\n",
    "    pct_high_risk_industry: float = 0.08,\n",
    "\n",
    "    # declared economics (rough priors)\n",
    "    avg_ticket_declared_mu: float = 30.0,\n",
    "    avg_ticket_declared_sigma: float = 0.5,   # lognormal sigma\n",
    "    monthly_volume_declared_mu: float = 75000.0,\n",
    "    monthly_volume_declared_sigma: float = 0.7,\n",
    "\n",
    "    # refund / chargeback policies\n",
    "    refund_policy_days_choices: tuple = (7, 14, 30),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create simple KYC records for each merchant_id.\n",
    "    Inputs:\n",
    "      - Provide either `merchant_ids`, or `transactions_df` (will infer unique merchants),\n",
    "        and optionally `merchant_meta` to reuse merchant_name/mcc/state from your generator.\n",
    "\n",
    "    Output:\n",
    "      One row per merchant with standard MA fields and random anomalies (flags).\n",
    "    \"\"\"\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "    # ---- resolve merchant universe ----\n",
    "    if merchant_ids is None:\n",
    "        if transactions_df is None:\n",
    "            raise ValueError(\"Provide either merchant_ids or transactions_df.\")\n",
    "        merchant_ids = (\n",
    "            transactions_df[[\"merchant_id\"]]\n",
    "            .dropna().drop_duplicates()[\"merchant_id\"].astype(str).tolist()\n",
    "        )\n",
    "    else:\n",
    "        merchant_ids = [str(x) for x in merchant_ids]\n",
    "\n",
    "    meta = None\n",
    "    if merchant_meta is not None:\n",
    "        meta = (merchant_meta.copy()\n",
    "                .drop_duplicates(subset=[\"merchant_id\"])\n",
    "                .set_index(\"merchant_id\"))\n",
    "    elif transactions_df is not None:\n",
    "        # try to harvest basic meta from transactions\n",
    "        cols = [c for c in [\"merchant_id\",\"merchant_name\",\"mcc\",\"mcc_description\",\"state\"] if c in transactions_df.columns]\n",
    "        if cols:\n",
    "            meta = (transactions_df[cols].dropna()\n",
    "                    .drop_duplicates(subset=[\"merchant_id\"])\n",
    "                    .set_index(\"merchant_id\"))\n",
    "\n",
    "    # ---- assign anomaly cohorts ----\n",
    "    def pick_subset(pct: float) -> set:\n",
    "        n = len(merchant_ids)\n",
    "        k = max(1, int(round(pct * n))) if n > 0 and pct > 0 else 0\n",
    "        return set(random.sample(merchant_ids, min(k, n))) if k > 0 else set()\n",
    "\n",
    "    cohort_sanctions  = pick_subset(pct_sanctioned_hit)\n",
    "    cohort_pep        = pick_subset(pct_pep_owner)\n",
    "    cohort_adverse    = pick_subset(pct_adverse_media)\n",
    "    cohort_missdocs   = pick_subset(pct_missing_docs)\n",
    "    cohort_bad_tax    = pick_subset(pct_incorrect_tax_id)\n",
    "    cohort_pobox      = pick_subset(pct_po_box_address)\n",
    "    cohort_highrisk   = pick_subset(pct_high_risk_industry)\n",
    "\n",
    "    rows = []\n",
    "    today = date.today()\n",
    "\n",
    "    for mid in merchant_ids:\n",
    "        # basic identity / naming\n",
    "        if meta is not None and mid in meta.index:\n",
    "            mname = str(meta.loc[mid].get(\"merchant_name\", _rand_business_name(mid))).strip() or _rand_business_name(mid)\n",
    "            state_from_tx = meta.loc[mid].get(\"state\", None)\n",
    "            mcc_code, mcc_desc = _choose_mcc(meta.loc[mid])\n",
    "        else:\n",
    "            mname = _rand_business_name(mid)\n",
    "            state_from_tx = None\n",
    "            mcc_code, mcc_desc = _choose_mcc(None)\n",
    "\n",
    "        street, city, st, zipc = _rand_address()\n",
    "        if state_from_tx and state_from_tx in US_STATES:\n",
    "            st = state_from_tx  # keep consistency with transactions if provided\n",
    "\n",
    "        # anomalies: PO Box address\n",
    "        if mid in cohort_pobox:\n",
    "            street = f\"PO Box {random.randint(10, 9999)}\"\n",
    "\n",
    "        # EIN (tax id)\n",
    "        ein_ok = mid not in cohort_bad_tax\n",
    "        tax_id = _rand_ein(correct=ein_ok)\n",
    "\n",
    "        # declared economics (simple priors)\n",
    "        avg_ticket_decl = float(np.round(np.random.lognormal(mean=np.log(avg_ticket_declared_mu), sigma=avg_ticket_declared_sigma), 2))\n",
    "        monthly_volume_decl = float(np.round(np.random.lognormal(mean=np.log(monthly_volume_declared_mu), sigma=monthly_volume_declared_sigma), 2))\n",
    "\n",
    "        # business attributes\n",
    "        business_type = random.choice([\"Sole Proprietor\",\"LLC\",\"Corporation\",\"Partnership\",\"Non-Profit\"])\n",
    "        incorporation_date = today - timedelta(days=random.randint(365*1, 365*25))\n",
    "\n",
    "        # ownership\n",
    "        bo_count = random.choice([1,2,3,4])\n",
    "        bo_pec = sorted(np.random.dirichlet(np.ones(bo_count)), reverse=True)\n",
    "        bo_top_share = float(np.round(bo_pec[0], 4))\n",
    "\n",
    "        # policies & contacts\n",
    "        refund_days = random.choice(refund_policy_days_choices)\n",
    "        phone = _rand_phone()\n",
    "        email = _rand_email(mname)\n",
    "        website = _rand_website(mname)\n",
    "\n",
    "        # risk & anomalies (flags as bool; you can cast to float if prefieres)\n",
    "        is_sanctioned_match = (mid in cohort_sanctions)\n",
    "        pep_beneficial_owner = (mid in cohort_pep)\n",
    "        adverse_media = (mid in cohort_adverse)\n",
    "        missing_kyc_docs = (mid in cohort_missdocs)\n",
    "        po_box_address = (mid in cohort_pobox)\n",
    "        incorrect_tax_id_format = (mid in cohort_bad_tax)\n",
    "        is_high_risk_industry = (mid in cohort_highrisk) or (mcc_code in HIGH_RISK_INDUSTRIES)\n",
    "\n",
    "        # simple risk score (0–100) combining signals\n",
    "        score = 20.0\n",
    "        score += 25.0 if is_sanctioned_match else 0.0\n",
    "        score += 15.0 if pep_beneficial_owner else 0.0\n",
    "        score += 10.0 if adverse_media else 0.0\n",
    "        score += 7.0  if missing_kyc_docs else 0.0\n",
    "        score += 5.0  if incorrect_tax_id_format else 0.0\n",
    "        score += 5.0  if po_box_address else 0.0\n",
    "        score += 8.0  if is_high_risk_industry else 0.0\n",
    "        risk_score = float(np.clip(score + np.random.normal(0, 3), 0, 100))\n",
    "\n",
    "        rows.append({\n",
    "            # keys\n",
    "            \"merchant_id\": str(mid),\n",
    "\n",
    "            # legal/commercial identity\n",
    "            \"legal_name\": mname.replace(\" LLC\",\"\").replace(\" Inc.\",\"\"),\n",
    "            \"business_name\": mname,\n",
    "            \"business_type\": business_type,\n",
    "            \"incorporation_date\": pd.to_datetime(incorporation_date).date(),\n",
    "\n",
    "            # registration / tax\n",
    "            \"tax_id_ein\": tax_id,\n",
    "\n",
    "            # geography / contact\n",
    "            \"country\": \"US\",\n",
    "            \"state\": st,\n",
    "            \"city\": city,\n",
    "            \"postal_code\": zipc,\n",
    "            \"address_line\": street,\n",
    "            \"phone_number\": phone,\n",
    "            \"email\": email,\n",
    "            \"website\": website,\n",
    "\n",
    "            # merchant category\n",
    "            \"mcc\": str(mcc_code),\n",
    "            \"mcc_description\": mcc_desc,\n",
    "\n",
    "            # declared economics\n",
    "            \"average_ticket_declared\": avg_ticket_decl,\n",
    "            \"monthly_volume_declared\": monthly_volume_decl,\n",
    "\n",
    "            # policy\n",
    "            \"refund_policy_days\": int(refund_days),\n",
    "            \"chargeback_contact_email\": f\"chargebacks@{re.sub(r'[^a-z0-9]+','', mname.lower())[:12] or 'merchant'}.com\",\n",
    "\n",
    "            # ownership summary\n",
    "            \"beneficial_owners_count\": int(bo_count),\n",
    "            \"top_owner_share\": bo_top_share,   # 0–1\n",
    "\n",
    "            # anomaly/risk flags\n",
    "            \"sanctioned_hit\": bool(is_sanctioned_match),\n",
    "            \"pep_beneficial_owner\": bool(pep_beneficial_owner),\n",
    "            \"adverse_media\": bool(adverse_media),\n",
    "            \"missing_kyc_docs\": bool(missing_kyc_docs),\n",
    "            \"incorrect_tax_id_format\": bool(incorrect_tax_id_format),\n",
    "            \"po_box_address\": bool(po_box_address),\n",
    "            \"high_risk_industry\": bool(is_high_risk_industry),\n",
    "\n",
    "            # score\n",
    "            \"risk_score\": float(risk_score),\n",
    "        })\n",
    "\n",
    "    df_kyc = pd.DataFrame(rows)\n",
    "\n",
    "    # enforce dtypes\n",
    "    str_cols = [\"merchant_id\",\"legal_name\",\"business_name\",\"business_type\",\"tax_id_ein\",\n",
    "                \"country\",\"state\",\"city\",\"postal_code\",\"address_line\",\"phone_number\",\n",
    "                \"email\",\"website\",\"mcc\",\"mcc_description\",\"chargeback_contact_email\"]\n",
    "    for c in str_cols:\n",
    "        df_kyc[c] = df_kyc[c].astype(\"object\")\n",
    "\n",
    "    df_kyc[\"incorporation_date\"] = pd.to_datetime(df_kyc[\"incorporation_date\"]).dt.date\n",
    "    df_kyc[\"refund_policy_days\"] = pd.to_numeric(df_kyc[\"refund_policy_days\"], errors=\"coerce\").astype(int)\n",
    "    df_kyc[\"beneficial_owners_count\"] = pd.to_numeric(df_kyc[\"beneficial_owners_count\"], errors=\"coerce\").astype(int)\n",
    "    df_kyc[\"top_owner_share\"] = pd.to_numeric(df_kyc[\"top_owner_share\"], errors=\"coerce\").astype(float)\n",
    "    df_kyc[\"average_ticket_declared\"] = pd.to_numeric(df_kyc[\"average_ticket_declared\"], errors=\"coerce\").astype(float)\n",
    "    df_kyc[\"monthly_volume_declared\"] = pd.to_numeric(df_kyc[\"monthly_volume_declared\"], errors=\"coerce\").astype(float)\n",
    "    df_kyc[\"risk_score\"] = (\n",
    "    pd.to_numeric(df_kyc[\"risk_score\"], errors=\"coerce\")  \n",
    "    .fillna(0)                                            \n",
    "    .astype(\"float64\")                               \n",
    "    .round(2)                                          \n",
    ")\n",
    "\n",
    "    bool_cols = [\"sanctioned_hit\",\"pep_beneficial_owner\",\"adverse_media\",\"missing_kyc_docs\",\n",
    "                 \"incorrect_tax_id_format\",\"po_box_address\",\"high_risk_industry\"]\n",
    "    for c in bool_cols:\n",
    "        df_kyc[c] = df_kyc[c].astype(bool)\n",
    "\n",
    "    return df_kyc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140318f6-2c7e-4dca-b6c1-4f6a0d0bbe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Si ya tienes tx de tu generador:\n",
    "merchant_ids = df_trx[\"merchant_id\"].dropna().unique().tolist()\n",
    "\n",
    "df_kyc_anomalos = generate_merchant_kyc(\n",
    "    merchant_ids=merchant_ids,     \n",
    "    transactions_df=df_trx,           \n",
    "    pct_sanctioned_hit=0.01,\n",
    "    pct_pep_owner=0.02,\n",
    "    pct_adverse_media=0.06,\n",
    "    pct_missing_docs=0.10,\n",
    "    pct_incorrect_tax_id=0.05,\n",
    "    pct_po_box_address=0.05,\n",
    "    pct_high_risk_industry=0.08\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1249c6-da65-4eb3-8508-bedcba13c6e4",
   "metadata": {},
   "source": [
    "# Write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80ef94-3d2c-41bc-947c-c2f02818b69c",
   "metadata": {},
   "source": [
    "### 1. Transactions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086725b-46f8-44b1-b15f-9572d623ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_functions.write(\n",
    "    context,\n",
    "    context.get_spark_session().createDataFrame(df_trx),\n",
    "    transactions_dataset().identifier,\n",
    "    data_environment=DataEnvironment.PUBLIC)\n",
    "\n",
    "dataset_functions.publish(context, \n",
    "                          transactions_dataset().identifier,\n",
    "                          data_environment=DataEnvironment.PUBLIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28863998-f038-4759-a829-0be93c207f33",
   "metadata": {},
   "source": [
    "### 2. Aggregate Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad8436-6d94-445d-91cd-595f72f0ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_functions.write(context, \n",
    "                        context.get_spark_session().createDataFrame(agg_df), \n",
    "                        customer_monthly_dataset().identifier,\n",
    "                        data_environment=DataEnvironment.PUBLIC)\n",
    "\n",
    "\n",
    "\n",
    "dataset_functions.publish(context, \n",
    "                          customer_monthly_dataset().identifier,\n",
    "                          data_environment=DataEnvironment.PUBLIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd41506-e8df-4002-8556-352e1eb28709",
   "metadata": {},
   "source": [
    "### 3. Anomalous KYC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687c160-ded7-4b61-a76b-34b974329eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_functions.write(context, \n",
    "                        context.get_spark_session().createDataFrame(df_kyc_anomalos),\n",
    "                        customers_dataset().identifier,\n",
    "                        data_environment=DataEnvironment.PUBLIC)\n",
    "\n",
    "dataset_functions.publish(context, \n",
    "                          customers_dataset().identifier,\n",
    "                          data_environment=DataEnvironment.PUBLIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc9b9a-14e0-48c8-8718-218eeca9b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
