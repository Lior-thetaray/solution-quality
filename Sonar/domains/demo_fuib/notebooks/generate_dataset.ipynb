{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import json\n",
    "import logging\n",
    "from pyspark.sql import DataFrame, Window, functions as f\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import LongType\n",
    "import yaml\n",
    "\n",
    "from common.libs import dates as dates_lib\n",
    "from common.libs import features_discovery\n",
    "from common.libs.features_executor import FeaturesExecutor\n",
    "from common.libs.feature_engineering import max_look_back_monthly_features, max_look_back_daily_weekly_features\n",
    "from common.libs.zscore import enrich_with_z_score\n",
    "from common.factory.wrangling_execution_strategy import get_wrangling_execution_strategy\n",
    "from common.factory.eval_flow_definition import get_evaluation_flow_definition\n",
    "from common.factory.domain_definition import get_domain_definition\n",
    "from common.notebook_utils.wrangling.wrangling_execution_strategy import WranglingExecutionStrategy\n",
    "from common.definitions.domain import DomainDefinition\n",
    "from common.definitions.eval_flow import EvaluationFlowDefinition\n",
    "from common.libs.context_utils import get_dataset\n",
    "\n",
    "from thetaray.api.context import init_context\n",
    "from thetaray.api.dataset import dataset_functions\n",
    "from thetaray.api.solution import IngestionMode\n",
    "from thetaray.common import Constants\n",
    "from thetaray.common.data_environment import DataEnvironment\n",
    "\n",
    "logging.getLogger().handlers[0].setFormatter(logging.Formatter(fmt='%(levelname)s: %(asctime)s @ %(message)s',datefmt='%Y-%m-%d %H:%M:%S'))\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "from thetaray.api.context import init_context\n",
    "import datetime\n",
    "from thetaray.common import Constants\n",
    "\n",
    "from common.libs.config.loader import load_config\n",
    "from common.libs.config.basic_execution_config_loader import BasicExecutionConfig, DevBasicExecutionConfig\n",
    "from common.libs.context_utils import is_run_triggered_from_airflow\n",
    "\n",
    "\n",
    "\n",
    "with open('/thetaray/git/solutions/domains/demo_fuib/config/spark_config.yaml') as spark_config_file:\n",
    "    spark_config = yaml.load(spark_config_file, yaml.FullLoader)['spark_config_a']\n",
    "\n",
    "execution_date=datetime.datetime(1970, 1, 1)\n",
    "\n",
    "context = init_context(domain='demo_fuib',\n",
    "                       execution_date=execution_date,\n",
    "                       spark_conf=spark_config,\n",
    "                       spark_master='local[*]')\n",
    "\n",
    "spark = context.get_spark_session()\n",
    "sc = SQLContext(spark)\n",
    "params = context.parameters\n",
    "print(f\"Spark UI URL: {context.get_spark_ui_url()}\")\n",
    "\n",
    "print(json.dumps(params, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from domains.demo_fuib.libs.transactions_generator_spark import generate_transactions_spark\n",
    "\n",
    "# df = generate_transactions_spark(\n",
    "#     spark,\n",
    "#     start_date=\"2023-07-01\",\n",
    "#     end_date=\"2025-06-30\",\n",
    "#     n_customers=1200,\n",
    "#     pct_anomalous_monthly=0.1,\n",
    "#     avg_txn_per_customer_per_month=(5, 60),\n",
    "#     seed=1337\n",
    "# )\n",
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window as W\n",
    "\n",
    "def build_train_and_pred_base(\n",
    "    spark,\n",
    "    *,\n",
    "    n_customers: int = 1500,\n",
    "    seed: int = 1337,\n",
    "    max_train_rows: int = 1_000_000,\n",
    "):\n",
    "    # 1) Generate once through June 30 (so the same customers exist in June)\n",
    "    from domains.demo_fuib.libs.transactions_generator_spark import generate_transactions_spark\n",
    "\n",
    "    df_all = generate_transactions_spark(\n",
    "        spark,\n",
    "        start_date=\"2023-07-01\",\n",
    "        end_date=\"2025-06-30\",\n",
    "        n_customers=n_customers,\n",
    "        pct_anomalous_monthly=0.00,                 # keep base anomalies very low (or 0.0)\n",
    "        avg_txn_per_customer_per_month=(5, 60),\n",
    "        seed=seed\n",
    "    ).cache()\n",
    "\n",
    "    # 2) Split into train (≤ 2025-05-31) and pred_base (2025-06)\n",
    "    train = df_all.where(F.col(\"txn_ts\") < F.lit(\"2025-06-01\"))\n",
    "    pred_base = df_all.where((F.col(\"txn_ts\") >= F.lit(\"2025-06-01\")) & (F.col(\"txn_ts\") < F.lit(\"2025-07-01\")))\n",
    "\n",
    "    # 3) Cap train rows to ≤ 1M with per-customer proportional sampling (no heavy skew)\n",
    "    cnt = train.count()\n",
    "    if cnt > max_train_rows:\n",
    "        # Compute per-customer quotas proportional to each customer's share in train\n",
    "        cust_sizes = train.groupBy(\"customer_id\").count().withColumnRenamed(\"count\", \"cust_cnt\")\n",
    "        total = cust_sizes.agg(F.sum(\"cust_cnt\")).first()[0]\n",
    "        quota = cust_sizes.withColumn(\n",
    "            \"keep\", (F.col(\"cust_cnt\") / F.lit(total) * F.lit(max_train_rows)).cast(\"int\")\n",
    "        )\n",
    "\n",
    "        # Assign a stable random order per customer and keep up to 'keep'\n",
    "        w = W.partitionBy(\"customer_id\").orderBy(F.rand(seed))\n",
    "        train_ranked = train.withColumn(\"rn\", F.row_number().over(w))\n",
    "\n",
    "        train = (\n",
    "            train_ranked.join(quota, \"customer_id\", \"left\")\n",
    "                        .where(F.col(\"rn\") <= F.greatest(F.col(\"keep\"), F.lit(1)))\n",
    "                        .drop(\"rn\", \"cust_cnt\", \"keep\")\n",
    "        )\n",
    "\n",
    "        # (Optional) exact trim if we were off by a few rows:\n",
    "        over = train.count() - max_train_rows\n",
    "        if over > 0:\n",
    "            train = train.orderBy(F.rand(seed + 1)).limit(max_train_rows)\n",
    "\n",
    "    return train.cache(), pred_base.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple, Optional, List\n",
    "from datetime import datetime, timedelta\n",
    "import random, string, numpy as np\n",
    "from pyspark.sql import DataFrame, Row, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "HIGH_RISK_DEST = {\"TR\", \"CY\", \"AE\"}\n",
    "FX_RATES_TO_USD = {\"UAH\": 0.027, \"USD\": 1.00, \"EUR\": 1.10}\n",
    "CURRENCIES = [\"UAH\", \"USD\", \"EUR\"]\n",
    "\n",
    "def _rand_id(prefix: str, n: int = 10) -> str:\n",
    "    s = \"\".join(random.choices(string.ascii_uppercase + string.digits, k=n))\n",
    "    return f\"{prefix}_{s}\"\n",
    "\n",
    "def _usd(amount: float, currency: str) -> float:\n",
    "    return float(round(amount * FX_RATES_TO_USD.get(currency, 1.0), 2))\n",
    "\n",
    "def _pick_currency_for_usd_target(usd_target: float) -> Tuple[str, float]:\n",
    "    c = random.choices(CURRENCIES, weights=[0.2,0.7,0.1], k=1)[0]\n",
    "    if c == \"USD\": return \"USD\", float(round(usd_target,2))\n",
    "    return c, float(round(usd_target / FX_RATES_TO_USD[c], 2))\n",
    "\n",
    "def _sample_times(start_dt: datetime, end_dt: datetime, n: int) -> List[datetime]:\n",
    "    span = int((end_dt - start_dt).total_seconds())\n",
    "    offs = sorted(np.random.randint(0, max(1, span+1), size=n).tolist())\n",
    "    return [start_dt + timedelta(seconds=int(o)) for o in offs]\n",
    "\n",
    "def _pick_3day_window(last_start: datetime, last_end: datetime) -> Tuple[datetime, datetime]:\n",
    "    latest_start = last_end - timedelta(days=3)\n",
    "    if latest_start <= last_start: s = last_start\n",
    "    else:\n",
    "        span = int((latest_start - last_start).total_seconds())\n",
    "        s = last_start + timedelta(seconds=int(np.random.randint(0, span+1)))\n",
    "    return s, s + timedelta(days=3)\n",
    "\n",
    "def _build_row(c: Dict[str, Any], ts: datetime, *, method: str, direction: str,\n",
    "               usd_amount: float, purpose: str = \"services\",\n",
    "               cp_id: Optional[str] = None, cp_jur: Optional[str] = None,\n",
    "               cross_border: bool = False, known_shell: bool = False,\n",
    "               same_cp: bool = False) -> Row:\n",
    "    currency, amount = _pick_currency_for_usd_target(usd_amount)\n",
    "    return Row(**{\n",
    "        \"txn_id\": _rand_id(\"T\", 12),\n",
    "        \"txn_ts\": ts,\n",
    "        \"customer_id\": c[\"customer_id\"],\n",
    "        \"account_id\": c[\"account_id\"],\n",
    "        # KYC snapshot\n",
    "        \"customer_type\": c.get(\"customer_type\"),\n",
    "        \"customer_name\": c.get(\"customer_name\"),\n",
    "        \"date_of_birth\": c.get(\"date_of_birth\"),\n",
    "        \"address_full\": c.get(\"address_full\"),\n",
    "        \"occupation\": c.get(\"occupation\"),\n",
    "        \"pep_indicator\": bool(c.get(\"pep_indicator\", False)),\n",
    "        \"industry_code\": c.get(\"industry_code\"),\n",
    "        \"account_open_months\": None,\n",
    "        \"whitelist_flag\": bool(c.get(\"whitelist_flag\", False)),\n",
    "        # Txn\n",
    "        \"direction\": direction,\n",
    "        \"method\": method,\n",
    "        \"amount\": float(amount),\n",
    "        \"currency\": currency,\n",
    "        \"amount_usd\": float(_usd(amount, currency)),\n",
    "        \"purpose_code\": purpose,\n",
    "        \"cross_border_flag\": bool(cross_border),\n",
    "        \"declared_counterparty_name\": None,\n",
    "        \"balance_after_txn\": 0.0,\n",
    "        \"is_split_component\": False,\n",
    "        # Counterparty\n",
    "        \"cp_id\": (cp_id if cp_id else _rand_id(\"CP\", 8)) if not same_cp else \"CP_ANCHOR\",\n",
    "        \"cp_type\": \"external_legal_entity\",\n",
    "        \"cp_jurisdiction\": cp_jur,\n",
    "        \"cp_known_shell_flag\": bool(known_shell),\n",
    "        \"cp_is_bank_customer\": False,\n",
    "        \"counterparty_name\": None,\n",
    "        \"counterparty_address\": None,\n",
    "        \"counterparty_ip\": None,\n",
    "        # Endpoint\n",
    "        \"endpoint_type\": \"online_banking\",\n",
    "        \"ip_hash\": f\"IP_{_rand_id('',6)[1:]}\",\n",
    "        \"device_id_hash\": f\"DEV_{_rand_id('',6)[1:]}\",\n",
    "        \"geo_lat\": None, \"geo_lon\": None, \"atm_id\": None,\n",
    "        \"withdrawal_channel\": None, \"pos_terminal_id\": None,\n",
    "        # Linkage/annotations\n",
    "        \"rep_id_hash\": c.get(\"rep_id_hash\"),\n",
    "        \"address_hash\": c.get(\"address_hash\"),\n",
    "        \"website_hash\": c.get(\"website_hash\"),\n",
    "        \"scenario_type\": \"injected_last_month_capped\",\n",
    "        \"scenario_role\": \"injector\",\n",
    "        \"anomaly_flag\": True,\n",
    "    })\n",
    "\n",
    "def inject_last_month_anomalies_proportional_capped(\n",
    "    spark: SparkSession,\n",
    "    *,\n",
    "    train_df: DataFrame,       # up to 2025-05-31\n",
    "    pred_base_df: DataFrame,   # June 2025 baseline\n",
    "    frac_customers: Dict[str, float],\n",
    "    multipliers: Dict[str, float],\n",
    "    max_injected_share_of_june: float = 0.10,  # ≤ 10% extra rows vs June baseline\n",
    "    seed: int = 1337\n",
    ") -> DataFrame:\n",
    "\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "    # Bounds for June\n",
    "    last_start = datetime(2025,6,1,0,0,0)\n",
    "    last_end   = datetime(2025,6,30,23,59,59)\n",
    "\n",
    "    # Ensure continuity: consider customers that exist in TRAIN and in June baseline\n",
    "    train_customers = train_df.select(\"customer_id\",\"account_id\",\"customer_type\",\"customer_name\",\n",
    "                                      \"date_of_birth\",\"address_full\",\"occupation\",\"pep_indicator\",\n",
    "                                      \"industry_code\",\"whitelist_flag\",\"rep_id_hash\",\"address_hash\",\"website_hash\") \\\n",
    "                              .dropDuplicates([\"customer_id\",\"account_id\"])\n",
    "    june_customers  = pred_base_df.select(\"customer_id\",\"account_id\").dropDuplicates([\"customer_id\",\"account_id\"])\n",
    "    active = june_customers.join(train_customers, [\"customer_id\",\"account_id\"], \"inner\")\n",
    "    n_active = active.count()\n",
    "    if n_active == 0:\n",
    "        return pred_base_df  # nothing to inject\n",
    "\n",
    "    # Baselines from previous 3 months (Mar/Apr/May 2025)\n",
    "    prev3 = train_df.where((F.col(\"txn_ts\") >= F.lit(\"2025-03-01\")) & (F.col(\"txn_ts\") < F.lit(\"2025-06-01\")))\n",
    "    prev3_agg = prev3.groupBy(\"customer_id\").agg(\n",
    "        F.sum(F.abs(\"amount_usd\")).alias(\"base_turnover_usd\"),\n",
    "        F.count(\"*\").alias(\"base_txn_count\"),\n",
    "        F.max(F.abs(\"amount_usd\")).alias(\"base_max_single_usd\"),\n",
    "        F.countDistinct(\"cp_id\").alias(\"base_distinct_cp\"),\n",
    "        F.sum(F.when(F.col(\"cp_jurisdiction\").isin(list(HIGH_RISK_DEST)),\n",
    "                     F.abs(F.col(\"amount_usd\"))).otherwise(0.0)).alias(\"base_turnover_hr_usd\")\n",
    "    )\n",
    "\n",
    "    june_agg = pred_base_df.groupBy(\"customer_id\").agg(\n",
    "        F.sum(F.abs(\"amount_usd\")).alias(\"cur_turnover_usd\"),\n",
    "        F.count(\"*\").alias(\"cur_txn_count\"),\n",
    "        F.max(F.abs(\"amount_usd\")).alias(\"cur_max_single_usd\"),\n",
    "        F.countDistinct(\"cp_id\").alias(\"cur_distinct_cp\"),\n",
    "        F.sum(F.when(F.col(\"cp_jurisdiction\").isin(list(HIGH_RISK_DEST)),\n",
    "                     F.abs(F.col(\"amount_usd\"))).otherwise(0.0)).alias(\"cur_turnover_hr_usd\")\n",
    "    )\n",
    "\n",
    "    stats = (active.join(june_agg, \"customer_id\", \"left\")\n",
    "                   .join(prev3_agg, \"customer_id\", \"left\")\n",
    "                   .fillna({\"cur_turnover_usd\":0.0,\"cur_txn_count\":0,\"cur_max_single_usd\":0.0,\"cur_distinct_cp\":0,\n",
    "                            \"base_turnover_usd\":0.0,\"base_txn_count\":0,\"base_max_single_usd\":0.0,\"base_distinct_cp\":0,\n",
    "                            \"cur_turnover_hr_usd\":0.0,\"base_turnover_hr_usd\":0.0}))\n",
    "\n",
    "    candidates = [r.asDict(recursive=True) for r in stats.collect()]\n",
    "    random.shuffle(candidates)\n",
    "\n",
    "    def pick_fraction(tag: str) -> List[Dict[str, Any]]:\n",
    "        k = max(1, int(float(frac_customers.get(tag, 0.0)) * n_active))\n",
    "        return random.sample(candidates, k=min(k, len(candidates)))\n",
    "\n",
    "    new_rows: List[Row] = []\n",
    "\n",
    "    # ---------- Patterns (inject deltas to reach targets) ----------\n",
    "    # 1) High monthly transactional value\n",
    "    for c in pick_fraction(\"high_monthly_value\"):\n",
    "        base = float(c[\"base_turnover_usd\"])/max(1,3)\n",
    "        target = max(30_000.0, multipliers.get(\"turnover\",3.0)*base)\n",
    "        delta = max(0.0, target - float(c[\"cur_turnover_usd\"]))\n",
    "        if delta <= 0: continue\n",
    "        n = int(np.random.randint(5,9))\n",
    "        parts = (np.random.dirichlet(np.ones(n)) * delta).tolist()\n",
    "        times = _sample_times(last_start, last_end, n)\n",
    "        for a,t in zip(parts,times):\n",
    "            new_rows.append(_build_row(c,t,method=\"wire_out\",direction=\"debit\",usd_amount=float(a),purpose=\"invoice\"))\n",
    "\n",
    "    # 2) High monthly value to high-risk jurisdictions\n",
    "    for c in pick_fraction(\"high_monthly_value_hr\"):\n",
    "        base = float(c[\"base_turnover_hr_usd\"])/max(1,3)\n",
    "        target = max(20_000.0, multipliers.get(\"turnover_hr\",2.5)*base)\n",
    "        delta = max(0.0, target - float(c[\"cur_turnover_hr_usd\"]))\n",
    "        if delta <= 0: continue\n",
    "        n = int(np.random.randint(4,8))\n",
    "        parts = (np.random.dirichlet(np.ones(n)) * delta).tolist()\n",
    "        times = _sample_times(last_start, last_end, n)\n",
    "        for a,t in zip(parts,times):\n",
    "            new_rows.append(_build_row(c,t,method=\"wire_out\",direction=\"debit\",usd_amount=float(a),\n",
    "                                       purpose=\"services\", cp_jur=random.choice(list(HIGH_RISK_DEST)),\n",
    "                                       cross_border=True, known_shell=(random.random()<0.3)))\n",
    "\n",
    "    # 3) High monthly transactional volume\n",
    "    for c in pick_fraction(\"high_monthly_volume\"):\n",
    "        base_cnt = float(c[\"base_txn_count\"])/max(1,3)\n",
    "        target_cnt = int(max(30, multipliers.get(\"volume\",3.0)*base_cnt))\n",
    "        need = max(0, target_cnt - int(c[\"cur_txn_count\"]))\n",
    "        if need <= 0: continue\n",
    "        times = _sample_times(last_start, last_end, need)\n",
    "        for t in times:\n",
    "            amt = float(np.random.lognormal(mean=3.0, sigma=0.4))  # small e-com\n",
    "            new_rows.append(_build_row(c,t,method=\"card_ecom\",direction=\"debit\",usd_amount=amt,purpose=\"small_purchase\"))\n",
    "\n",
    "    # 4) High single transactional value\n",
    "    for c in pick_fraction(\"high_single_txn\"):\n",
    "        base_max = float(c[\"base_max_single_usd\"])\n",
    "        target = max(20_000.0, multipliers.get(\"single_txn\",3.0)*base_max)\n",
    "        if float(c[\"cur_max_single_usd\"]) >= target: continue\n",
    "        t = _sample_times(last_start, last_end, 1)[0]\n",
    "        new_rows.append(_build_row(c,t,method=\"wire_out\",direction=\"debit\",usd_amount=target,purpose=\"one_off_capital\"))\n",
    "\n",
    "    # 5) High transactional value in a 3-day window\n",
    "    for c in pick_fraction(\"high_value_3d\"):\n",
    "        base = float(c[\"base_turnover_usd\"])/max(1,3)\n",
    "        target_cluster = max(25_000.0, multipliers.get(\"value_3d\",2.5)*base)\n",
    "        s,e = _pick_3day_window(last_start,last_end)\n",
    "        n = int(np.random.randint(5,9))\n",
    "        parts = (np.random.dirichlet(np.ones(n)) * target_cluster).tolist()\n",
    "        times = _sample_times(s,e,n)\n",
    "        for a,t in zip(parts,times):\n",
    "            new_rows.append(_build_row(c,t,method=\"wire_out\",direction=\"debit\",usd_amount=float(a),purpose=\"clustered_payouts\"))\n",
    "\n",
    "    # 6) High number of counterparties\n",
    "    for c in pick_fraction(\"many_counterparties\"):\n",
    "        base_k = float(c[\"base_distinct_cp\"])/max(1,3)\n",
    "        target_k = int(max(10, multipliers.get(\"distinct_cp\",3.0)*base_k))\n",
    "        need_k = max(0, target_k - int(c[\"cur_distinct_cp\"]))\n",
    "        if need_k <= 0: continue\n",
    "        times = _sample_times(last_start, last_end, need_k)\n",
    "        for t in times:\n",
    "            amt = float(np.random.lognormal(9.5,0.4)/1000)\n",
    "            new_rows.append(_build_row(c,t,method=\"wire_out\",direction=\"debit\",\n",
    "                                       usd_amount=amt,purpose=\"supplier\",cp_id=_rand_id(\"CP\",8)))\n",
    "\n",
    "    # 7) High concentration to a single counterparty\n",
    "    for c in pick_fraction(\"concentration_one_cp\"):\n",
    "        target_share = float(multipliers.get(\"concentration_share\",0.85))\n",
    "        base = float(c[\"base_turnover_usd\"])/max(1,3)\n",
    "        anchor = _rand_id(\"CP\",8)\n",
    "        n = int(np.random.randint(25,45))\n",
    "        chunk = max(20_000.0, 0.8*target_share*base)\n",
    "        times = _sample_times(last_start,last_end,n)\n",
    "        for t in times:\n",
    "            usd_amt = (chunk / n) * float(np.random.uniform(0.8,1.2))\n",
    "            new_rows.append(_build_row(c,t,method=\"wire_out\",direction=\"debit\",\n",
    "                                       usd_amount=usd_amt,purpose=\"services\",\n",
    "                                       cp_id=anchor, same_cp=True))\n",
    "\n",
    "    # 8) Similar high credit and debit in a 3-day window (fast in/out)\n",
    "    for c in pick_fraction(\"fast_in_fast_out\"):\n",
    "        base = float(c[\"base_turnover_usd\"])/max(1,3)\n",
    "        s,e = _pick_3day_window(last_start,last_end)\n",
    "        t_in = _sample_times(s, e - timedelta(hours=24), 1)[0]\n",
    "        base_in = max(25_000.0, 0.6*base)\n",
    "        # credit in\n",
    "        new_rows.append(_build_row(c,t_in,method=\"wire_in\",direction=\"credit\",\n",
    "                                   usd_amount=base_in,purpose=\"unexpected_inflow\"))\n",
    "        # one or two debits totaling ≈ credit (±5%)\n",
    "        k = int(np.random.randint(1,3))\n",
    "        out_total = base_in * float(np.random.uniform(0.95,1.05))\n",
    "        weights = np.random.dirichlet(np.ones(k))\n",
    "        for w in weights:\n",
    "            t_out = t_in + timedelta(hours=int(np.random.randint(6,48)))\n",
    "            new_rows.append(_build_row(c,t_out,method=\"wire_out\",direction=\"debit\",\n",
    "                                       usd_amount=float(out_total*w),purpose=\"rapid_outflow\"))\n",
    "\n",
    "    # ---------- Hard cap on injected rows ----------\n",
    "    june_rows = pred_base_df.count()\n",
    "    cap = int(june_rows * max_injected_share_of_june)  # e.g., ≤10% extra rows\n",
    "    if len(new_rows) > cap:\n",
    "        random.shuffle(new_rows)\n",
    "        new_rows = new_rows[:cap]\n",
    "\n",
    "    if not new_rows:\n",
    "        return pred_base_df\n",
    "\n",
    "    injected_df = spark.createDataFrame(new_rows, schema=pred_base_df.schema)\n",
    "    return pred_base_df.unionByName(injected_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: build train (≤1M) and June baseline\n",
    "train, pred_base = build_train_and_pred_base(spark, n_customers=1500, seed=1337, max_train_rows=800000)\n",
    "\n",
    "print(\"TRAIN rows:\", train.count())\n",
    "print(\"JUNE baseline rows:\", pred_base.count())\n",
    "\n",
    "# Step 2: inject strong anomalies into June, capped\n",
    "frac_customers = {\n",
    "    \"high_monthly_value\": 0.05,\n",
    "    \"high_monthly_value_hr\": 0.05,\n",
    "    \"high_monthly_volume\": 0.05,\n",
    "    \"high_single_txn\": 0.05,\n",
    "    \"high_value_3d\": 0.05,\n",
    "    \"many_counterparties\": 0.05,\n",
    "    \"concentration_one_cp\": 0.05,\n",
    "    \"fast_in_fast_out\": 0.05,\n",
    "}\n",
    "multipliers = {\n",
    "    \"turnover\": 5.0,\n",
    "    \"turnover_hr\": 4.5,\n",
    "    \"volume\": 4.0,\n",
    "    \"single_txn\": 4.0,\n",
    "    \"value_3d\": 4.0,\n",
    "    \"distinct_cp\": 4.0,\n",
    "    \"concentration_share\": 0.95,\n",
    "}\n",
    "\n",
    "pred = inject_last_month_anomalies_proportional_capped(\n",
    "    spark,\n",
    "    train_df=train,\n",
    "    pred_base_df=pred_base,\n",
    "    frac_customers=frac_customers,\n",
    "    multipliers=multipliers,\n",
    "    max_injected_share_of_june=0.25,   # ≤ 10% more rows than June baseline\n",
    "    seed=1337\n",
    ")\n",
    "\n",
    "print(\"PRED rows (after injection):\", pred.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# # Add a source label\n",
    "# train_labeled = train.withColumn(\"dataset_type\", F.lit(\"train\"))\n",
    "# pred_labeled  = pred.withColumn(\"dataset_type\", F.lit(\"pred\"))\n",
    "\n",
    "# # Combine into one DataFrame\n",
    "# df = train_labeled.unionByName(pred_labeled)\n",
    "\n",
    "# # Optional: cache if you'll query it a lot\n",
    "# df.cache()\n",
    "\n",
    "# print(\"Combined count:\", df.count())\n",
    "# df.groupBy(\"dataset_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, functions as F\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "import random, string\n",
    "\n",
    "# --- Config ---\n",
    "TOTAL_IN_UAH = 450_000\n",
    "OUT_UAH      = 420_000\n",
    "FX_UAH_TO_USD = 0.027  # keep consistent with your generator\n",
    "DOB_2007 = date(2007, 4, 15)   # any date in 2007 works\n",
    "THREE_DAY_START = datetime(2025, 6, 10, 10, 0, 0)  # anchor for the 3-day window (change if you like)\n",
    "\n",
    "def _rand_id(prefix: str, n: int = 10) -> str:\n",
    "    s = \"\".join(random.choices(string.ascii_uppercase + string.digits, k=n))\n",
    "    return f\"{prefix}_{s}\"\n",
    "\n",
    "def _sample_times_3d(start_dt: datetime, n: int):\n",
    "    end_dt = start_dt + timedelta(days=3)\n",
    "    span = int((end_dt - start_dt).total_seconds())\n",
    "    offs = sorted(np.random.randint(0, max(1, span+1), size=n).tolist())\n",
    "    return [start_dt + timedelta(seconds=int(o)) for o in offs]\n",
    "\n",
    "def inject_student_case(pred_df, spark, target_customer_id=None, seed=2025):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "    schema = pred_df.schema\n",
    "\n",
    "    # 1) Find a June-active individual customer if not provided\n",
    "    if target_customer_id is None:\n",
    "        june_inds = (\n",
    "            pred_df\n",
    "            .where((F.col(\"txn_ts\") >= F.lit(\"2025-06-01\")) & (F.col(\"txn_ts\") < F.lit(\"2025-07-01\")))\n",
    "            .where(F.col(\"customer_type\") == \"individual\")\n",
    "            .select(\"customer_id\", \"account_id\", \"customer_type\", \"customer_name\",\n",
    "                    \"date_of_birth\", \"address_full\", \"occupation\", \"pep_indicator\",\n",
    "                    \"industry_code\", \"whitelist_flag\", \"rep_id_hash\", \"address_hash\", \"website_hash\")\n",
    "            .dropDuplicates([\"customer_id\",\"account_id\"])\n",
    "            .limit(1)\n",
    "            .collect()\n",
    "        )\n",
    "        if not june_inds:\n",
    "            # fallback: any customer\n",
    "            june_inds = (\n",
    "                pred_df.select(\"customer_id\",\"account_id\",\"customer_type\",\"customer_name\",\n",
    "                               \"date_of_birth\",\"address_full\",\"occupation\",\"pep_indicator\",\n",
    "                               \"industry_code\",\"whitelist_flag\",\"rep_id_hash\",\"address_hash\",\"website_hash\")\n",
    "                       .dropDuplicates([\"customer_id\",\"account_id\"])\n",
    "                       .limit(1)\n",
    "                       .collect()\n",
    "            )\n",
    "        base = june_inds[0].asDict(recursive=True)\n",
    "    else:\n",
    "        # Pull a snapshot row for this customer from pred (June)\n",
    "        base = (\n",
    "            pred_df.where(F.col(\"customer_id\")==F.lit(target_customer_id))\n",
    "                   .select(\"customer_id\",\"account_id\",\"customer_type\",\"customer_name\",\n",
    "                           \"date_of_birth\",\"address_full\",\"occupation\",\"pep_indicator\",\n",
    "                           \"industry_code\",\"whitelist_flag\",\"rep_id_hash\",\"address_hash\",\"website_hash\")\n",
    "                   .limit(1).collect()\n",
    "        )\n",
    "        if not base:\n",
    "            raise ValueError(f\"customer_id {target_customer_id} not found in pred_df\")\n",
    "        base = base[0].asDict(recursive=True)\n",
    "\n",
    "    # Force KYC snapshot for this case\n",
    "    base[\"customer_type\"] = \"individual\"\n",
    "    base[\"occupation\"] = \"student\"\n",
    "    base[\"date_of_birth\"] = DOB_2007  # string is OK; Spark will cast to DateType if schema expects it\n",
    "\n",
    "    # 2) Build 15 inbound wires ~450k UAH total\n",
    "    n_in = 15\n",
    "    times_in = _sample_times_3d(THREE_DAY_START, n_in)\n",
    "    # Split the total into 15 positive parts with small variation\n",
    "    parts = np.random.dirichlet(np.ones(n_in)) * TOTAL_IN_UAH\n",
    "    amounts_in_uah = [float(round(a, 2)) for a in parts]  # keep decimals if you want; can cast to int\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for i, (ts, amt_uah) in enumerate(zip(times_in, amounts_in_uah)):\n",
    "        rows.append(Row(**{\n",
    "            \"txn_id\": _rand_id(\"T\", 12),\n",
    "            \"txn_ts\": ts,\n",
    "            \"customer_id\": base[\"customer_id\"],\n",
    "            \"account_id\": base[\"account_id\"],\n",
    "            # KYC snapshot (per-row in your schema)\n",
    "            \"customer_type\": base.get(\"customer_type\"),\n",
    "            \"customer_name\": base.get(\"customer_name\"),\n",
    "            \"date_of_birth\": base.get(\"date_of_birth\"),\n",
    "            \"address_full\": base.get(\"address_full\"),\n",
    "            \"occupation\": base.get(\"occupation\"),\n",
    "            \"pep_indicator\": bool(base.get(\"pep_indicator\", False)),\n",
    "            \"industry_code\": base.get(\"industry_code\"),\n",
    "            \"account_open_months\": None,\n",
    "            \"whitelist_flag\": bool(base.get(\"whitelist_flag\", False)),\n",
    "            # Txn\n",
    "            \"direction\": \"credit\",\n",
    "            \"method\": \"wire_in\",\n",
    "            \"amount\": amt_uah,\n",
    "            \"currency\": \"UAH\",\n",
    "            \"amount_usd\": round(amt_uah * FX_UAH_TO_USD, 2),\n",
    "            \"purpose_code\": \"tuition_support\",\n",
    "            \"cross_border_flag\": False,\n",
    "            \"declared_counterparty_name\": None,\n",
    "            \"balance_after_txn\": 0.0,\n",
    "            \"is_split_component\": False,\n",
    "            # Counterparty (distinct cp_id each time)\n",
    "            \"cp_id\": _rand_id(\"CP\", 8),\n",
    "            \"cp_type\": \"external_individual\",\n",
    "            \"cp_jurisdiction\": \"UA\",\n",
    "            \"cp_known_shell_flag\": False,\n",
    "            \"cp_is_bank_customer\": False,\n",
    "            \"counterparty_name\": None,\n",
    "            \"counterparty_address\": None,\n",
    "            \"counterparty_ip\": None,\n",
    "            # Endpoint (online banking is fine)\n",
    "            \"endpoint_type\": \"online_banking\",\n",
    "            \"ip_hash\": f\"IP_{_rand_id('',6)[1:]}\",\n",
    "            \"device_id_hash\": f\"DEV_{_rand_id('',6)[1:]}\",\n",
    "            \"geo_lat\": None, \"geo_lon\": None, \"atm_id\": None,\n",
    "            \"withdrawal_channel\": None, \"pos_terminal_id\": None,\n",
    "            # Linkage\n",
    "            \"rep_id_hash\": base.get(\"rep_id_hash\"),\n",
    "            \"address_hash\": base.get(\"address_hash\"),\n",
    "            \"website_hash\": base.get(\"website_hash\"),\n",
    "            # Annotations\n",
    "            \"scenario_type\": \"case_student_cluster\",\n",
    "            \"scenario_role\": \"subject\",\n",
    "            \"anomaly_flag\": True,\n",
    "        }))\n",
    "\n",
    "    # 3) One outbound wire to Turkey ~420k UAH within the 3-day window (after last inbound)\n",
    "    t_out = times_in[-1] + timedelta(hours=int(np.random.randint(4, 24)))\n",
    "    rows.append(Row(**{\n",
    "        \"txn_id\": _rand_id(\"T\", 12),\n",
    "        \"txn_ts\": t_out,\n",
    "        \"customer_id\": base[\"customer_id\"],\n",
    "        \"account_id\": base[\"account_id\"],\n",
    "        # KYC snapshot\n",
    "        \"customer_type\": base.get(\"customer_type\"),\n",
    "        \"customer_name\": base.get(\"customer_name\"),\n",
    "        \"date_of_birth\": base.get(\"date_of_birth\"),\n",
    "        \"address_full\": base.get(\"address_full\"),\n",
    "        \"occupation\": base.get(\"occupation\"),\n",
    "        \"pep_indicator\": bool(base.get(\"pep_indicator\", False)),\n",
    "        \"industry_code\": base.get(\"industry_code\"),\n",
    "        \"account_open_months\": None,\n",
    "        \"whitelist_flag\": bool(base.get(\"whitelist_flag\", False)),\n",
    "        # Txn\n",
    "        \"direction\": \"debit\",\n",
    "        \"method\": \"wire_out\",\n",
    "        \"amount\": float(OUT_UAH),\n",
    "        \"currency\": \"UAH\",\n",
    "        \"amount_usd\": round(OUT_UAH * FX_UAH_TO_USD, 2),\n",
    "        \"purpose_code\": \"study_expense\",\n",
    "        \"cross_border_flag\": True,\n",
    "        \"declared_counterparty_name\": \"Overseas Tuition\",\n",
    "        \"balance_after_txn\": 0.0,\n",
    "        \"is_split_component\": False,\n",
    "        # Counterparty (Turkey = high risk in your mapping)\n",
    "        \"cp_id\": _rand_id(\"CP\", 8),\n",
    "        \"cp_type\": \"foreign_bank\",\n",
    "        \"cp_jurisdiction\": \"TR\",\n",
    "        \"cp_known_shell_flag\": False,\n",
    "        \"cp_is_bank_customer\": False,\n",
    "        \"counterparty_name\": \"TR_Beneficiary\",\n",
    "        \"counterparty_address\": None,\n",
    "        \"counterparty_ip\": None,\n",
    "        # Endpoint\n",
    "        \"endpoint_type\": \"online_banking\",\n",
    "        \"ip_hash\": f\"IP_{_rand_id('',6)[1:]}\",\n",
    "        \"device_id_hash\": f\"DEV_{_rand_id('',6)[1:]}\",\n",
    "        \"geo_lat\": None, \"geo_lon\": None, \"atm_id\": None,\n",
    "        \"withdrawal_channel\": None, \"pos_terminal_id\": None,\n",
    "        # Linkage\n",
    "        \"rep_id_hash\": base.get(\"rep_id_hash\"),\n",
    "        \"address_hash\": base.get(\"address_hash\"),\n",
    "        \"website_hash\": base.get(\"website_hash\"),\n",
    "        # Annotations\n",
    "        \"scenario_type\": \"case_student_cluster\",\n",
    "        \"scenario_role\": \"subject\",\n",
    "        \"anomaly_flag\": True,\n",
    "    }))\n",
    "\n",
    "    injected = spark.createDataFrame(rows, schema=schema)\n",
    "    # injected  = injected.withColumn(\"dataset_type\", F.lit(\"pred\"))\n",
    "    \n",
    "    return pred_df.unionByName(injected), base[\"customer_id\"]\n",
    "\n",
    "# --------- Run it ---------\n",
    "pred_with_case, student_customer_id = inject_student_case(pred, spark)\n",
    "print(\"Injected student case for customer:\", student_customer_id)\n",
    "print(pred_with_case.count())\n",
    "\n",
    "# Quick verification\n",
    "from pyspark.sql import functions as F\n",
    "pred_with_case.where(\n",
    "    (F.col(\"customer_id\")==student_customer_id) &\n",
    "    (F.col(\"scenario_type\")==\"case_student_cluster\")\n",
    ").select(\n",
    "    \"txn_ts\",\"direction\",\"method\",\"currency\",\"amount\",\"amount_usd\",\n",
    "    \"cp_jurisdiction\",\"purpose_code\"\n",
    ").orderBy(\"txn_ts\").show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Add a source label\n",
    "train_labeled = train.withColumn(\"dataset_type\", F.lit(\"train\"))\n",
    "pred_labeled  = pred_with_case.withColumn(\"dataset_type\", F.lit(\"pred\"))\n",
    "\n",
    "# Combine into one DataFrame\n",
    "df = train_labeled.unionByName(pred_labeled)\n",
    "\n",
    "# Optional: cache if you'll query it a lot\n",
    "df.cache()\n",
    "\n",
    "print(\"Combined count:\", df.count())\n",
    "df.groupBy(\"dataset_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the mapping dictionary\n",
    "COUNTRY_RISK = {\n",
    "    \"UA\": \"Medium\",\n",
    "    \"PL\": \"Low\",\n",
    "    \"TR\": \"High\",\n",
    "    \"CY\": \"High\",\n",
    "    \"AE\": \"Medium\",\n",
    "    \"GB\": \"Low\",\n",
    "    \"DE\": \"Low\",\n",
    "    \"US\": \"Low\",\n",
    "    \"LT\": \"Low\"\n",
    "}\n",
    "\n",
    "# Add the cp_country_risk_level column\n",
    "df = df.withColumn(\n",
    "    \"cp_country_risk_level\",\n",
    "    f.create_map([f.lit(x) for kv in COUNTRY_RISK.items() for x in kv])\n",
    "      .getItem(F.col(\"cp_jurisdiction\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"is_new_account\",\n",
    "    f.when(f.col('account_open_months')<=6, f.lit(True)).otherwise(f.lit(False))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.libs import dates as dates_lib\n",
    "\n",
    "trx_date_column_name = \"txn_ts\"\n",
    "df = dates_lib.add_day_offset_column(df, trx_date_column_name, 'day_offset')\n",
    "df = dates_lib.add_month_offset_column(df, trx_date_column_name, 'month_offset')\n",
    "df = dates_lib.month_offset_to_year_month_columns(df, 'month_offset', 'year_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from domains.common.libs.tr_levenshtein import get_lev_ind\n",
    "#Me to me\n",
    "threshold = 1\n",
    "df = df.withColumn(\"is_me_to_me\", get_lev_ind('customer_name', 'counterparty_name', threshold))\n",
    "print(\"Me-to-me field added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from thetaray.utils.schema_ds_ffc_generator import create_metadata_ds_file_from_df, create_metadata_ds_file_from_csv\n",
    "# from thetaray.api.solution import DataSet, Field, DataType, IngestionMode\n",
    "\n",
    "# create_metadata_ds_file_from_df(context=context,\n",
    "#                                 df=df,\n",
    "#                                 ds_identifier=\"trx_enriched\",\n",
    "#                                 ds_display_name=\"trx_enriched\",\n",
    "#                                 ingestion_mode=IngestionMode.APPEND,\n",
    "#                                 publish=True,\n",
    "#                                 primary_key=['txn_id'],\n",
    "#                                 occurred_on_field=\"txn_ts\",\n",
    "#                                 data_permission=\"dpv:demo_fuib\",\n",
    "#                                 num_of_partitions=1,\n",
    "#                                 num_of_buckets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thetaray.common.data_environment import DataEnvironment\n",
    "dataset_functions.write(context, df, \"trx_enriched\", data_environment=DataEnvironment.PUBLIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
