{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a937226b-535a-404f-96b2-ed13ffa2102e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from thetaray.api.context import init_context\n",
    "from thetaray.api.dataset import dataset_functions\n",
    "from thetaray.common.data_environment import DataEnvironment\n",
    "from thetaray.api.graph import publish_nodes, publish_edges\n",
    "from thetaray.api.evaluation import read_alerted_activities, load_evaluated_activities\n",
    "import random\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from thetaray.common import Constants\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cffb715-9a54-4b40-8947-50c99a6f3023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from common.libs.config.loader import load_config\n",
    "from thetaray.api.context import init_context\n",
    "import datetime\n",
    "from thetaray.common import Constants\n",
    "\n",
    "from common.libs.config.loader import load_config\n",
    "from common.libs.config.basic_execution_config_loader import BasicExecutionConfig, DevBasicExecutionConfig\n",
    "from common.libs.context_utils import is_run_triggered_from_airflow\n",
    "\n",
    "import datetime\n",
    "import dateutil.relativedelta\n",
    "import json\n",
    "import logging\n",
    "from pyspark.sql import DataFrame, Window, functions as f\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.functions import col, when,count\n",
    "from common.libs import dates as dates_lib\n",
    "from common.libs import features_discovery\n",
    "from thetaray.common.data_environment import DataEnvironment\n",
    "from common.libs.features_executor import FeaturesExecutor\n",
    "from common.libs.feature_engineering import max_look_back_monthly_features, max_look_back_daily_weekly_features\n",
    "from common.libs.zscore import enrich_with_z_score\n",
    "from common.factory.wrangling_execution_strategy import get_wrangling_execution_strategy\n",
    "from common.factory.eval_flow_definition import get_evaluation_flow_definition\n",
    "from common.factory.domain_definition import get_domain_definition\n",
    "from common.notebook_utils.wrangling.wrangling_execution_strategy import WranglingExecutionStrategy\n",
    "from common.definitions.domain import DomainDefinition\n",
    "from common.definitions.eval_flow import EvaluationFlowDefinition\n",
    "from common.libs.context_utils import get_dataset\n",
    "\n",
    "from thetaray.api.context import init_context\n",
    "from thetaray.api.dataset import dataset_functions\n",
    "from thetaray.api.solution import IngestionMode\n",
    "from thetaray.common import Constants\n",
    "from thetaray.common.data_environment import DataEnvironment\n",
    "\n",
    "logging.getLogger().handlers[0].setFormatter(logging.Formatter(fmt='%(levelname)s: %(asctime)s @ %(message)s',datefmt='%Y-%m-%d %H:%M:%S'))\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from common.libs.config.loader import load_config\n",
    "from thetaray.api.context import init_context\n",
    "import datetime\n",
    "from thetaray.common import Constants\n",
    "\n",
    "from common.libs.config.loader import load_config\n",
    "from common.libs.config.basic_execution_config_loader import BasicExecutionConfig, DevBasicExecutionConfig\n",
    "from common.libs.context_utils import is_run_triggered_from_airflow\n",
    "\n",
    "execution_date = Constants.BEGINNING_OF_TIME\n",
    "\n",
    "if is_run_triggered_from_airflow():\n",
    "    context = init_context()\n",
    "    basic_execution_config = BasicExecutionConfig(domain=context.domain,\n",
    "                                                  stage=context.parameters['stage'],\n",
    "                                                  entity=context.parameters['entity'],\n",
    "                                                  cadence=context.parameters['cadence'],\n",
    "                                                  spark_conf=context.spark_conf)\n",
    "else:\n",
    "    basic_execution_config = DevBasicExecutionConfig()\n",
    "    context = init_context(execution_date=execution_date,\n",
    "                           domain=basic_execution_config.domain,\n",
    "                           spark_conf=basic_execution_config.spark_conf)\n",
    "\n",
    "\n",
    "print(basic_execution_config)\n",
    "spark = context.get_spark_session()\n",
    "sc = SQLContext(spark)\n",
    "params = context.parameters\n",
    "print(f\"Spark UI URL: {context.get_spark_ui_url()}\")\n",
    "print(json.dumps(params, indent=4))\n",
    "\n",
    "domain = basic_execution_config.domain\n",
    "entity = basic_execution_config.entity\n",
    "cadence = basic_execution_config.cadence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60445098-646c-4827-b44f-d861487faa2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from common.libs.network_visualization import create_alert_graph_entities\n",
    "from common.factory.eval_flow_definition import get_evaluation_flow_definition, EvaluationFlowDefinition\n",
    "\n",
    "eval_flow_def: EvaluationFlowDefinition = get_evaluation_flow_definition(basic_execution_config)\n",
    "\n",
    "evaluation_flow_identifier = eval_flow_def.eval_flow_id\n",
    "\n",
    "alerted_activities_df = read_alerted_activities(context, evaluation_flow_identifier,data_environment=DataEnvironment.PUBLIC)\n",
    "evaluated_activities_df = load_evaluated_activities(context, evaluation_flow_identifier,data_environment=DataEnvironment.PUBLIC)\n",
    "alert_nodes_df, alert_edges_df = create_alert_graph_entities(alerted_activities_df=alerted_activities_df,\n",
    "                                                             evaluated_activities_df=evaluated_activities_df,\n",
    "                                                             entity_id_column_name=eval_flow_def.investigated_entity_id_column_name,\n",
    "                                                             date_column_name=eval_flow_def.date_column_name)\n",
    "alert_nodes_df=alert_nodes_df.withColumnRenamed('activity_id', 'AI')\n",
    "alert_nodes_df=alert_nodes_df.withColumnRenamed('risk_id', 'RI')\n",
    "alert_nodes_df=alert_nodes_df.withColumnRenamed('suppressed', 'SP')\n",
    "publish_nodes(context=context, nodes_df=alert_nodes_df, graph_identifier='graph', node_type='AL',data_environment=DataEnvironment.PUBLIC)\n",
    "publish_edges(context=context, edges_df=alert_edges_df, graph_identifier='graph', edge_type='AL', \n",
    "              source_node_type='AL', target_node_type='AC', data_environment=DataEnvironment.PUBLIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263d070-ccf1-477a-b26d-6c5f3e96d9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
