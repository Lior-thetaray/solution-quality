{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19cebc9-333a-4fc0-9b77-f1d2c8669baf",
   "metadata": {},
   "source": [
    "### Init Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040bf097-c103-4473-86e3-f50ce1face7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thetaray.api.context import init_context\n",
    "import datetime\n",
    "import yaml\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(message)s')\n",
    "\n",
    "with open('/thetaray/git/solutions/domains/demo_remittance/config/spark_config.yaml') as spark_config_file:\n",
    "    spark_config = yaml.load(spark_config_file, yaml.FullLoader)['spark_config_a']\n",
    "context = init_context(execution_date=datetime.datetime(1970, 2, 1),\n",
    "                       spark_conf=spark_config,\n",
    "                       spark_master='local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089821e6-a478-49e1-b1db-d1965dcafd88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T12:45:25.256221Z",
     "iopub.status.busy": "2025-05-06T12:45:25.255592Z",
     "iopub.status.idle": "2025-05-06T12:45:25.258312Z",
     "shell.execute_reply": "2025-05-06T12:45:25.257873Z",
     "shell.execute_reply.started": "2025-05-06T12:45:25.256198Z"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeaaf18-0599-4319-9d9d-1a3f396ef784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thetaray.api.dataset import dataset_functions\n",
    "\n",
    "from domains.demo_remittance.datasets.customers import customers_dataset\n",
    "from domains.demo_remittance.datasets.transactions import transactions_dataset\n",
    "from domains.demo_remittance.datasets.customer_insights import customer_insights_dataset\n",
    "from domains.demo_remittance.evaluation_flows.ef import evaluation_flow\n",
    "\n",
    "import json\n",
    "import psycopg2\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from faker import Faker\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from thetaray.api.dataset.schema import DatasetSchemaHandler\n",
    "from thetaray.common import Constants, Settings\n",
    "from thetaray.common.data_environment import DataEnvironment\n",
    "\n",
    "spark = context.get_spark_session()\n",
    "\n",
    "ns_suffix = Settings.SHARED_NAMESPACE.removeprefix('shared-')\n",
    "\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a2b4f-d48e-45d2-9e33-e8dd29dda2c0",
   "metadata": {},
   "source": [
    "### Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb54097-bb69-44b9-86bd-cfd6a6ad8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_HOST = Settings.DB_HOST\n",
    "\n",
    "DB_USER_CDD = os.environ['CDD_POSTGRES_USERNAME']\n",
    "DB_PASS_CDD = os.environ['CDD_POSTGRES_PASSWORD']\n",
    "DB_USER_RP = 'postgres'\n",
    "DB_PASS_RP = 'postgres'\n",
    "\n",
    "\n",
    "dsn_cdd = (\n",
    "    f'user={DB_USER_CDD} '\n",
    "    f'password={DB_PASS_CDD} '\n",
    "    f'dbname={Constants.CDD_DB_NAME} '\n",
    "    f'host={DB_HOST[:-5]} '\n",
    "    f'port={DB_HOST[-4:]} '\n",
    "    'sslmode=verify-ca '\n",
    "    'sslrootcert=/certs/ca.crt'\n",
    ")\n",
    "\n",
    "\n",
    "dsn_rp = (\n",
    "    f'user={DB_USER_RP} '\n",
    "    f'password={DB_PASS_RP} '\n",
    "    f'dbname={Constants.CDD_DB_NAME} '\n",
    "    f'host={DB_HOST[:-5]} '\n",
    "    f'port={DB_HOST[-4:]} '\n",
    "    'sslmode=verify-ca '\n",
    "    'sslrootcert=/certs/ca.crt'\n",
    ")\n",
    "\n",
    "\n",
    "def execute_query(query, dsn):\n",
    "    conn = psycopg2.connect(dsn=dsn)\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        columns = [col.name for col in cursor.description]\n",
    "        rows = []\n",
    "        for row in cursor.fetchall():\n",
    "            rows.append({col: val for col, val in zip(columns, row)})\n",
    "        return rows\n",
    "\n",
    "def get_alert_mapper(solution, ef_id):\n",
    "    schema = f'apps_{ns_suffix.replace(\"-\", \"_\")}'\n",
    "    for alert_mapper in execute_query(f'SELECT * FROM {schema}.rp_mappers', dsn_rp):\n",
    "        ef_unit = json.loads(alert_mapper['solution_evaluation_flow_unit'])\n",
    "        if not ef_unit:\n",
    "            continue\n",
    "        ef_unit = ef_unit[0]\n",
    "        if ef_unit['solutionId'] == solution and ef_unit['evaluationFlowId'] == ef_id:\n",
    "            return alert_mapper\n",
    "\n",
    "\n",
    "def get_alerts(solution, ef_id):\n",
    "    schema = f'apps_{ns_suffix.replace(\"-\", \"_\")}'\n",
    "    alert_mapper = get_alert_mapper(solution, ef_id)\n",
    "    if alert_mapper is None:\n",
    "        raise Exception(f'Alert mapper not found for {solution = } and {ef_id = }')\n",
    "    alert_mapper_identifier = alert_mapper['identifier']\n",
    "    alert_fields = execute_query(f'SELECT * FROM {schema}.rp_alert_fields', dsn_rp)\n",
    "    alert_fields = {alert_field['rp_alert_id']: alert_field for alert_field in alert_fields}\n",
    "    alerts = execute_query(f\"SELECT * FROM {schema}.rp_alerts WHERE alert_mapper_identifier = '{alert_mapper_identifier}' AND history_type = 'CURRENT'\", dsn_rp)\n",
    "    for alert in alerts:\n",
    "        alert['customer_id'] = alert_fields[alert['alert_id']]['customer_id']\n",
    "    return alerts\n",
    "\n",
    "\n",
    "def get_accounts(solution):\n",
    "    schema = Constants.SOLUTION_SCHEMA_TPL.format(solution=solution)\n",
    "    query = f\"SELECT * FROM {schema}.demo_remittance_customers\"\n",
    "    return execute_query(query, dsn_cdd)\n",
    "\n",
    "\n",
    "def get_account_records(solution, customer_id):\n",
    "    schema = Constants.SOLUTION_SCHEMA_TPL.format(solution=solution)\n",
    "    query = f\"SELECT * FROM {schema}.demo_remittance_customers WHERE customer_id = '{customer_id}'\"\n",
    "    return execute_query(query, dsn_cdd)\n",
    "\n",
    "\n",
    "def get_account_transactions(solution, customer_id):\n",
    "    schema = Constants.SOLUTION_SCHEMA_TPL.format(solution=solution)\n",
    "    query = f\"SELECT * FROM {schema}.demo_remittance_transactions WHERE customer_id = '{customer_id}'\"\n",
    "    return execute_query(query, dsn_cdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2de590-92ee-496c-9c4f-57cbc6aa0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = Settings.SOLUTION\n",
    "ef_id = evaluation_flow().identifier\n",
    "schema = Constants.SOLUTION_SCHEMA_TPL.format(solution=solution)\n",
    "\n",
    "alerts = get_alerts(solution, ef_id)\n",
    "accounts = get_accounts(solution)\n",
    "\n",
    "account_country = execute_query(f'SELECT * FROM {schema}.demo_remittance_customers', dsn_cdd)\n",
    "account_country = {a['customer_id']: a['country_of_residence_code'] for a in account_country}\n",
    "\n",
    "customer_insights_data = []\n",
    "\n",
    "for account in accounts:\n",
    "    account_id = account['customer_id']\n",
    "    account_records = get_account_records(solution, account_id)\n",
    "    account = sorted(account_records, key=lambda x: x['tr_job_ts'])[-1]  # most recent account data\n",
    "    account_transactions = get_account_transactions(solution, account_id)\n",
    "    account_transactions_in = [trx for trx in account_transactions if trx['in_out'] == 'IN']\n",
    "    account_transactions_out = [trx for trx in account_transactions if trx['in_out'] == 'OUT']\n",
    "    account_alerts = [alert for alert in alerts if alert['customer_id'] == account_id]\n",
    "\n",
    "    kyc_classification = 'Medium'\n",
    "    kyc_name = account['name']\n",
    "    kyc_is_new = len(account_records) == 1\n",
    "    kyc_recently_updated = not kyc_is_new\n",
    "    kyc_newly_incorporation = random.choice([True, False])\n",
    "    kyc_occupation = account['occupation']\n",
    "    kyc_null_field = None\n",
    "\n",
    "    account_transactions_pd = pd.DataFrame(account_transactions)\n",
    "    hr_cc = [*set(account_transactions_pd.loc[account_transactions_pd.counterparty_country_risk == 'High']['counterparty_country'].tolist())]\n",
    "    mr_cc = [*set(account_transactions_pd.loc[account_transactions_pd.counterparty_country_risk == 'Medium']['counterparty_country'].tolist())]\n",
    "    lr_cc = [*set(account_transactions_pd.loc[account_transactions_pd.counterparty_country_risk == 'Low']['counterparty_country'].tolist())]\n",
    "\n",
    "    director_ad = {'CC': (cc := account['country_of_residence_code']), 'AD': account['address'], 'CL': 'L'}\n",
    "    director_ad = json.dumps(director_ad)\n",
    "    company_ad = director_ad\n",
    "    company_ad = json.dumps(company_ad)\n",
    "\n",
    "    tr_in = float(sum(trx['amount'] for trx in account_transactions_in))\n",
    "    tr_out = float(sum(trx['amount'] for trx in account_transactions_out))\n",
    "    tr_in_count = len(account_transactions_in)\n",
    "    tr_out_count = len(account_transactions_out)\n",
    "    tr_in_seg = tr_in * random.uniform(0.4, 0.8)\n",
    "    tr_out_seg = tr_out * random.uniform(0.4, 0.8)\n",
    "    tr_in_seg_count = int(tr_in_count * random.uniform(0.4, 0.8))\n",
    "    tr_out_seg_count = int(tr_out_count * random.uniform(0.4, 0.8))\n",
    "    trx_from_date = min(map(lambda trx: trx['transaction_timestamp'], account_transactions))\n",
    "    trx_to_date = max(map(lambda trx: trx['transaction_timestamp'], account_transactions))\n",
    "\n",
    "    tm_open = len([alert for alert in account_alerts if alert['state_id'] != 'state_closed'])\n",
    "    tm_closed = len([alert for alert in account_alerts if alert['state_id'] == 'state_closed'])\n",
    "    tm_false_positives = len([alert for alert in account_alerts if alert['resolution_code'] == 'Non_Issue'])\n",
    "    tm = {'Open': tm_open, 'Closed': tm_closed, 'False_positives': tm_false_positives}\n",
    "    tm = json.dumps(tm)\n",
    "\n",
    "    scrn_open = 0\n",
    "    scrn_closed = 0\n",
    "    scrn_false_positives = 0\n",
    "    scrn = {'Open': scrn_open, 'Closed': scrn_closed, 'False_positives': scrn_false_positives}\n",
    "    scrn = json.dumps(scrn)\n",
    "\n",
    "    account_insights_data = {\n",
    "        'customer_id': account_id,\n",
    "        'kyc_classification': kyc_classification,\n",
    "        'kyc_name': kyc_name,\n",
    "        'kyc_is_new': kyc_is_new,\n",
    "        'kyc_recently_updated': kyc_recently_updated,\n",
    "        'kyc_newly_incorporation': kyc_newly_incorporation,\n",
    "        'kyc_occupation': kyc_occupation,\n",
    "        'kyc_null_field': kyc_null_field,\n",
    "        'hr_cc': hr_cc,\n",
    "        'mr_cc': mr_cc,\n",
    "        'lr_cc': lr_cc,\n",
    "        'director_ad': director_ad,\n",
    "        'company_ad': company_ad,\n",
    "        'tr_in': tr_in,\n",
    "        'tr_out': tr_out,\n",
    "        'tr_in_count': tr_in_count,\n",
    "        'tr_out_count': tr_out_count,\n",
    "        'tr_in_seg': tr_in_seg,\n",
    "        'tr_out_seg': tr_out_seg,\n",
    "        'tr_in_seg_count': tr_in_seg_count,\n",
    "        'tr_out_seg_count': tr_out_seg_count,\n",
    "        'trx_from_date': trx_from_date,\n",
    "        'trx_to_date': trx_to_date,\n",
    "        'tm': tm,\n",
    "        'scrn': scrn\n",
    "    }\n",
    "\n",
    "    customer_insights_data.append(account_insights_data)\n",
    "\n",
    "\n",
    "customer_insights_ds = next(ds for ds in context.solution.datasets if ds.identifier == 'demo_remittance_customer_insights')\n",
    "customer_insights_schema = DatasetSchemaHandler(customer_insights_ds, context, data_environment=DataEnvironment.get_default())._build_dataset_schema()\n",
    "customer_insights_schema = StructType([s for s in customer_insights_schema if s.name in [f.identifier for f in customer_insights_ds.field_list]])\n",
    "customer_insights_df = spark.createDataFrame(customer_insights_data, schema=customer_insights_schema)\n",
    "customer_insights_df = customer_insights_df.withColumn('tr_timestamp', f.lit(context.execution_date))\n",
    "customer_insights_df = customer_insights_df.withColumn('effective_date', f.lit(context.execution_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e498f41-ad9c-4d2c-9399-f643ba4cabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_functions.write(context, customer_insights_df, customer_insights_dataset().identifier)\n",
    "dataset_functions.publish(context, customer_insights_dataset().identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff4a06-1fec-4f21-a60d-b758a671794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
