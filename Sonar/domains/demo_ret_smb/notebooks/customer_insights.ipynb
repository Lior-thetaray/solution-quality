{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19cebc9-333a-4fc0-9b77-f1d2c8669baf",
   "metadata": {},
   "source": [
    "### Init Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040bf097-c103-4473-86e3-f50ce1face7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T08:18:10.401707Z",
     "iopub.status.busy": "2025-06-03T08:18:10.400955Z",
     "iopub.status.idle": "2025-06-03T08:18:20.686484Z",
     "shell.execute_reply": "2025-06-03T08:18:20.685648Z",
     "shell.execute_reply.started": "2025-06-03T08:18:10.401676Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 08:18:20,214:INFO:thetaray.common.logging:start loading solution.....[ load_risks=True , solution_path=/thetaray/git/solutions/domains , settings_path=/thetaray/git/solutions/settings ]\n",
      "2025-06-03 08:18:20,232:WARNING:thetaray.common.logging:Encryption is enabled on dataset demo_ret_smb_customer_insights, but data encryption is disabled in deployment\n",
      "2025-06-03 08:18:20,251:WARNING:thetaray.common.logging:Encryption is enabled on dataset demo_ret_indiv_customer_insights, but data encryption is disabled in deployment\n",
      "2025-06-03 08:18:20,304:WARNING:thetaray.common.logging:Property AN is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,304:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,304:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,304:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,304:WARNING:thetaray.common.logging:Node AC is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,304:WARNING:thetaray.common.logging:Property PI is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,304:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,304:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,304:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,304:WARNING:thetaray.common.logging:Node PR is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Property AN is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Node AC is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Property PI is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Property NM is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Property AD is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Property CT is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Node PR is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,305:WARNING:thetaray.common.logging:Edge ER is configured as encrypted, but data encryption is disabled\n",
      "2025-06-03 08:18:20,355:INFO:thetaray.common.logging:load_risks took: 0.048140525817871094\n",
      "2025-06-03 08:18:20,357:INFO:thetaray.common.logging:Skip _connectors validation due to no connectors provided.\n",
      "2025-06-03 08:18:20,358:ERROR:thetaray.common.logging:failed to load solution\n",
      "Traceback (most recent call last):\n",
      "  File \"/thetaray/platform/python/thetaray/api/solution/loader.py\", line 95, in _load_solution\n",
      "    smd = SolutionMetaData(**smd_dict)\n",
      "  File \"/thetaray/venv/lib64/python3.9/site-packages/pydantic/_internal/_dataclasses.py\", line 124, in __init__\n",
      "    s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for SolutionMetaData\n",
      "  Value error, Pairs of widgets and fields that were not found in dataset demo_ret_smb_customer_insights for evaluation flow demo_ret_smb_ef:(kyc: ['kyc_occupation']) [type=value_error, input_value=ArgsKwargs((), {'connecto...rmation_threshold=0.8)}), input_type=ArgsKwargs]\n",
      "    For further information visit https://errors.pydantic.dev/2.3/v/value_error\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"/thetaray/platform/python/thetaray/api/solution/loader.py\", line 95, in _load_solution\n    smd = SolutionMetaData(**smd_dict)\n  File \"/thetaray/venv/lib64/python3.9/site-packages/pydantic/_internal/_dataclasses.py\", line 124, in __init__\n    s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\npydantic_core._pydantic_core.ValidationError: 1 validation error for SolutionMetaData\n  Value error, Pairs of widgets and fields that were not found in dataset demo_ret_smb_customer_insights for evaluation flow demo_ret_smb_ef:(kyc: ['kyc_occupation']) [type=value_error, input_value=ArgsKwargs((), {'connecto...rmation_threshold=0.8)}), input_type=ArgsKwargs]\n    For further information visit https://errors.pydantic.dev/2.3/v/value_error\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_703/1739070041.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/thetaray/git/solutions/domains/demo_ret_smb/config/spark_config.yaml'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspark_config_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mspark_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spark_config_a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m context = init_context(execution_date=datetime.datetime(1970, 2, 1),\n\u001b[0m\u001b[1;32m     11\u001b[0m                        \u001b[0mspark_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                        spark_master='local[*]')\n",
      "\u001b[0;32m/thetaray/platform/python/thetaray/api/context/context.py\u001b[0m in \u001b[0;36minit_context\u001b[0;34m(job_name, execution_date, mlflow_run_id, spark_conf, spark_master, solution, domain, delete_unused_columns, allow_type_changes, drop_undefined_datasets, skip_jars)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msolution\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;31m# serialized_context will be injected as a global variable in case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/thetaray/platform/python/thetaray/api/solution/loader.py\u001b[0m in \u001b[0;36mload_solution\u001b[0;34m(context, load_risks, solution_path)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Solution loaded successfully, digest: {result.md5_digest()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"/thetaray/platform/python/thetaray/api/solution/loader.py\", line 95, in _load_solution\n    smd = SolutionMetaData(**smd_dict)\n  File \"/thetaray/venv/lib64/python3.9/site-packages/pydantic/_internal/_dataclasses.py\", line 124, in __init__\n    s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\npydantic_core._pydantic_core.ValidationError: 1 validation error for SolutionMetaData\n  Value error, Pairs of widgets and fields that were not found in dataset demo_ret_smb_customer_insights for evaluation flow demo_ret_smb_ef:(kyc: ['kyc_occupation']) [type=value_error, input_value=ArgsKwargs((), {'connecto...rmation_threshold=0.8)}), input_type=ArgsKwargs]\n    For further information visit https://errors.pydantic.dev/2.3/v/value_error\n"
     ]
    }
   ],
   "source": [
    "from thetaray.api.context import init_context\n",
    "import datetime\n",
    "import yaml\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(message)s')\n",
    "\n",
    "with open('/thetaray/git/solutions/domains/demo_ret_smb/config/spark_config.yaml') as spark_config_file:\n",
    "    spark_config = yaml.load(spark_config_file, yaml.FullLoader)['spark_config_a']\n",
    "context = init_context(execution_date=datetime.datetime(1970, 2, 1),\n",
    "                       spark_conf=spark_config,\n",
    "                       spark_master='local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089821e6-a478-49e1-b1db-d1965dcafd88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T12:45:25.256221Z",
     "iopub.status.busy": "2025-05-06T12:45:25.255592Z",
     "iopub.status.idle": "2025-05-06T12:45:25.258312Z",
     "shell.execute_reply": "2025-05-06T12:45:25.257873Z",
     "shell.execute_reply.started": "2025-05-06T12:45:25.256198Z"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeaaf18-0599-4319-9d9d-1a3f396ef784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thetaray.api.dataset import dataset_functions\n",
    "\n",
    "from domains.demo_ret_smb.datasets.customers import customers_dataset\n",
    "from domains.demo_ret_smb.datasets.transactions import transactions_dataset\n",
    "from domains.demo_ret_smb.datasets.customer_insights import customer_insights_dataset\n",
    "from domains.demo_ret_smb.evaluation_flows.ef import evaluation_flow\n",
    "\n",
    "import json\n",
    "import psycopg2\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from faker import Faker\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from thetaray.api.dataset.schema import DatasetSchemaHandler\n",
    "from thetaray.common import Constants, Settings\n",
    "from thetaray.common.data_environment import DataEnvironment\n",
    "\n",
    "spark = context.get_spark_session()\n",
    "\n",
    "ns_suffix = Settings.SHARED_NAMESPACE.removeprefix('shared-')\n",
    "\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a2b4f-d48e-45d2-9e33-e8dd29dda2c0",
   "metadata": {},
   "source": [
    "### Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb54097-bb69-44b9-86bd-cfd6a6ad8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_HOST = Settings.DB_HOST\n",
    "\n",
    "DB_USER_CDD = os.environ['CDD_POSTGRES_USERNAME']\n",
    "DB_PASS_CDD = os.environ['CDD_POSTGRES_PASSWORD']\n",
    "DB_USER_RP = 'postgres'\n",
    "DB_PASS_RP = 'postgres'\n",
    "\n",
    "\n",
    "dsn_cdd = (\n",
    "    f'user={DB_USER_CDD} '\n",
    "    f'password={DB_PASS_CDD} '\n",
    "    f'dbname={Constants.CDD_DB_NAME} '\n",
    "    f'host={DB_HOST[:-5]} '\n",
    "    f'port={DB_HOST[-4:]} '\n",
    "    'sslmode=verify-ca '\n",
    "    'sslrootcert=/certs/ca.crt'\n",
    ")\n",
    "\n",
    "\n",
    "dsn_rp = (\n",
    "    f'user={DB_USER_RP} '\n",
    "    f'password={DB_PASS_RP} '\n",
    "    f'dbname={Constants.CDD_DB_NAME} '\n",
    "    f'host={DB_HOST[:-5]} '\n",
    "    f'port={DB_HOST[-4:]} '\n",
    "    'sslmode=verify-ca '\n",
    "    'sslrootcert=/certs/ca.crt'\n",
    ")\n",
    "\n",
    "\n",
    "def execute_query(query, dsn):\n",
    "    conn = psycopg2.connect(dsn=dsn)\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        columns = [col.name for col in cursor.description]\n",
    "        rows = []\n",
    "        for row in cursor.fetchall():\n",
    "            rows.append({col: val for col, val in zip(columns, row)})\n",
    "        return rows\n",
    "\n",
    "def get_alert_mapper(solution, ef_id):\n",
    "    schema = f'apps_{ns_suffix.replace(\"-\", \"_\")}'\n",
    "    for alert_mapper in execute_query(f'SELECT * FROM {schema}.rp_mappers', dsn_rp):\n",
    "        ef_unit = json.loads(alert_mapper['solution_evaluation_flow_unit'])\n",
    "        if not ef_unit:\n",
    "            continue\n",
    "        ef_unit = ef_unit[0]\n",
    "        if ef_unit['solutionId'] == solution and ef_unit['evaluationFlowId'] == ef_id:\n",
    "            return alert_mapper\n",
    "\n",
    "\n",
    "def get_alerts(solution, ef_id):\n",
    "    schema = f'apps_{ns_suffix.replace(\"-\", \"_\")}'\n",
    "    alert_mapper = get_alert_mapper(solution, ef_id)\n",
    "    if alert_mapper is None:\n",
    "        raise Exception(f'Alert mapper not found for {solution = } and {ef_id = }')\n",
    "    alert_mapper_identifier = alert_mapper['identifier']\n",
    "    alert_fields = execute_query(f'SELECT * FROM {schema}.rp_alert_fields', dsn_rp)\n",
    "    alert_fields = {alert_field['rp_alert_id']: alert_field for alert_field in alert_fields}\n",
    "    alerts = execute_query(f\"SELECT * FROM {schema}.rp_alerts WHERE alert_mapper_identifier = '{alert_mapper_identifier}' AND history_type = 'CURRENT'\", dsn_rp)\n",
    "    for alert in alerts:\n",
    "        alert['customer_id'] = alert_fields[alert['alert_id']]['customer_id']\n",
    "    return alerts\n",
    "\n",
    "\n",
    "def get_accounts(solution):\n",
    "    schema = Constants.SOLUTION_SCHEMA_TPL.format(solution=solution)\n",
    "    query = f\"SELECT * FROM {schema}.demo_ret_smb_customers\"\n",
    "    return execute_query(query, dsn_cdd)\n",
    "\n",
    "\n",
    "def get_account_records(solution, customer_id):\n",
    "    schema = Constants.SOLUTION_SCHEMA_TPL.format(solution=solution)\n",
    "    query = f\"SELECT * FROM {schema}.demo_ret_smb_customers WHERE customer_id = '{customer_id}'\"\n",
    "    return execute_query(query, dsn_cdd)\n",
    "\n",
    "\n",
    "def get_account_transactions(solution, customer_id):\n",
    "    schema = Constants.SOLUTION_SCHEMA_TPL.format(solution=solution)\n",
    "    query = f\"SELECT * FROM {schema}.demo_ret_smb_transactions WHERE customer_id = '{customer_id}'\"\n",
    "    return execute_query(query, dsn_cdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2de590-92ee-496c-9c4f-57cbc6aa0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = Settings.SOLUTION\n",
    "ef_id = evaluation_flow().identifier\n",
    "schema = Constants.SOLUTION_SCHEMA_TPL.format(solution=solution)\n",
    "\n",
    "alerts = get_alerts(solution, ef_id)\n",
    "accounts = get_accounts(solution)\n",
    "\n",
    "account_country = execute_query(f'SELECT * FROM {schema}.demo_ret_smb_customers', dsn_cdd)\n",
    "account_country = {a['customer_id']: a['incorporation_country'] for a in account_country}\n",
    "\n",
    "customer_insights_data = []\n",
    "\n",
    "for account in accounts:\n",
    "    account_id = account['customer_id']\n",
    "    account_records = get_account_records(solution, account_id)\n",
    "    account = sorted(account_records, key=lambda x: x['tr_job_ts'])[-1]  # most recent account data\n",
    "    account_transactions = get_account_transactions(solution, account_id)\n",
    "    account_transactions_in = [trx for trx in account_transactions if trx['in_out'] == 'IN']\n",
    "    account_transactions_out = [trx for trx in account_transactions if trx['in_out'] == 'OUT']\n",
    "    account_alerts = [alert for alert in alerts if alert['customer_id'] == account_id]\n",
    "\n",
    "    kyc_classification = 'Medium'\n",
    "    kyc_name = account['business_name']\n",
    "    kyc_is_new = len(account_records) == 1\n",
    "    kyc_recently_updated = not kyc_is_new\n",
    "    kyc_newly_incorporation = random.choice([True, False])\n",
    "    kyc_industry = account['industry']\n",
    "    kyc_null_field = None\n",
    "\n",
    "    account_transactions_pd = pd.DataFrame(account_transactions)\n",
    "    hr_cc = [*set(account_transactions_pd.loc[account_transactions_pd.counterparty_country_risk == 'High']['counterparty_country'].tolist())]\n",
    "    mr_cc = [*set(account_transactions_pd.loc[account_transactions_pd.counterparty_country_risk == 'Medium']['counterparty_country'].tolist())]\n",
    "    lr_cc = [*set(account_transactions_pd.loc[account_transactions_pd.counterparty_country_risk == 'Low']['counterparty_country'].tolist())]\n",
    "\n",
    "    director_ad = {'CC': (cc := account['incorporation_country']), 'AD': '', 'CL': 'L'}\n",
    "    director_ad = json.dumps(director_ad)\n",
    "    company_ad = director_ad\n",
    "    company_ad = json.dumps(company_ad)\n",
    "\n",
    "    tr_in = sum(trx['reference_trx_amount'] for trx in account_transactions_in)\n",
    "    tr_out = sum(trx['reference_trx_amount'] for trx in account_transactions_out)\n",
    "    tr_in_count = len(account_transactions_in)\n",
    "    tr_out_count = len(account_transactions_out)\n",
    "    tr_in_seg = tr_in * random.uniform(0.4, 0.8)\n",
    "    tr_out_seg = tr_out * random.uniform(0.4, 0.8)\n",
    "    tr_in_seg_count = int(tr_in_count * random.uniform(0.4, 0.8))\n",
    "    tr_out_seg_count = int(tr_out_count * random.uniform(0.4, 0.8))\n",
    "    trx_from_date = min(map(lambda trx: trx['transaction_timestamp'], account_transactions))\n",
    "    trx_to_date = max(map(lambda trx: trx['transaction_timestamp'], account_transactions))\n",
    "\n",
    "    tm_open = len([alert for alert in account_alerts if alert['state_id'] != 'state_closed'])\n",
    "    tm_closed = len([alert for alert in account_alerts if alert['state_id'] == 'state_closed'])\n",
    "    tm_false_positives = len([alert for alert in account_alerts if alert['resolution_code'] == 'Non_Issue'])\n",
    "    tm = {'Open': tm_open, 'Closed': tm_closed, 'False_positives': tm_false_positives}\n",
    "    tm = json.dumps(tm)\n",
    "\n",
    "    scrn_open = 0\n",
    "    scrn_closed = 0\n",
    "    scrn_false_positives = 0\n",
    "    scrn = {'Open': scrn_open, 'Closed': scrn_closed, 'False_positives': scrn_false_positives}\n",
    "    scrn = json.dumps(scrn)\n",
    "\n",
    "    account_insights_data = {\n",
    "        'customer_id': account_id,\n",
    "        'kyc_classification': kyc_classification,\n",
    "        'kyc_name': kyc_name,\n",
    "        'kyc_is_new': kyc_is_new,\n",
    "        'kyc_recently_updated': kyc_recently_updated,\n",
    "        'kyc_newly_incorporation': kyc_newly_incorporation,\n",
    "        'kyc_industry': kyc_industry,\n",
    "        'kyc_null_field': kyc_null_field,\n",
    "        'hr_cc': hr_cc,\n",
    "        'mr_cc': mr_cc,\n",
    "        'lr_cc': lr_cc,\n",
    "        'director_ad': director_ad,\n",
    "        'company_ad': company_ad,\n",
    "        'tr_in': tr_in,\n",
    "        'tr_out': tr_out,\n",
    "        'tr_in_count': tr_in_count,\n",
    "        'tr_out_count': tr_out_count,\n",
    "        'tr_in_seg': tr_in_seg,\n",
    "        'tr_out_seg': tr_out_seg,\n",
    "        'tr_in_seg_count': tr_in_seg_count,\n",
    "        'tr_out_seg_count': tr_out_seg_count,\n",
    "        'trx_from_date': trx_from_date,\n",
    "        'trx_to_date': trx_to_date,\n",
    "        'tm': tm,\n",
    "        'scrn': scrn\n",
    "    }\n",
    "\n",
    "    customer_insights_data.append(account_insights_data)\n",
    "\n",
    "\n",
    "customer_insights_ds = next(ds for ds in context.solution.datasets if ds.identifier == 'demo_ret_smb_customer_insights')\n",
    "customer_insights_schema = DatasetSchemaHandler(customer_insights_ds, context, data_environment=DataEnvironment.get_default())._build_dataset_schema()\n",
    "customer_insights_schema = StructType([s for s in customer_insights_schema if s.name in [f.identifier for f in customer_insights_ds.field_list]])\n",
    "customer_insights_df = spark.createDataFrame(customer_insights_data, schema=customer_insights_schema)\n",
    "customer_insights_df = customer_insights_df.withColumn('tr_timestamp', f.lit(context.execution_date))\n",
    "customer_insights_df = customer_insights_df.withColumn('effective_date', f.lit(context.execution_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e498f41-ad9c-4d2c-9399-f643ba4cabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_functions.write(context, customer_insights_df, customer_insights_dataset().identifier)\n",
    "dataset_functions.publish(context, customer_insights_dataset().identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff4a06-1fec-4f21-a60d-b758a671794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
